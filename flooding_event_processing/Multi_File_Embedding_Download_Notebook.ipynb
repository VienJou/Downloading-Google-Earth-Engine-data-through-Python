{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-File AlphaEarth Embedding Download Notebook (Improved Version)\n",
        "\n",
        "This notebook downloads AlphaEarth embeddings for **only the first 3 CSV files** with different temporal strategies, processing **only the first 3 features** from each file:\n",
        "\n",
        "## Files and Strategies (Limited to 3 files):\n",
        "\n",
        "1. **Unified_Peak_Data_2016_2017_with_ID(1006).csv** → embedding_1Y_later (download 2017, 2018) - **3 samples only**\n",
        "2. **Unified_Peak_Data_2018_and_later_with_ID(1006).csv** → embedding_1Y_early (download previous year) - **3 samples only**\n",
        "3. **matched_records_1947_with_ID_2016_2017(1006).csv** → embedding_1Y_later (download 2017, 2018) - **3 samples only**\n",
        "\n",
        "## Improvements:\n",
        "- **Adaptive region sizing**: Starts with 250x250 pixels, reduces to 128x128 or 64x64 if needed\n",
        "- **Retry mechanism**: Up to 3 attempts per feature with different strategies\n",
        "- **Better error handling**: Detailed logging and specific error type detection\n",
        "- **Sequential processing**: Avoids GEE quota issues\n",
        "- **Data availability diagnosis**: Tests AlphaEarth data availability before processing\n",
        "\n",
        "## Data Format:\n",
        "- **Structure**: 64×H×W arrays (64 bands A00-A63)\n",
        "- **Naming**: gage_[ID] for Unified Peak Data, HWM_[ID] for matched records\n",
        "- **Output**: Compressed .npz files with metadata\n",
        "- **Limit**: Maximum 3 features per file (9 total features)\n",
        "\n",
        "## Authentication:\n",
        "Uses service account: `zhouwenlc@windy-winter-456502-b1.iam.gserviceaccount.com`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dateutil import parser\n",
        "import time\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_gee():\n",
        "    \"\"\"Initialize Google Earth Engine using service account\"\"\"\n",
        "    try:\n",
        "        # Use existing service account credentials\n",
        "        SERVICE_ACCOUNT = 'zhouwenlc@windy-winter-456502-b1.iam.gserviceaccount.com'\n",
        "        KEY_FILE = '/u/wz53/Flooding/Data_acquire/downloaded_files/windy-winter-456502-b1-e3f770db867c.json'\n",
        "        \n",
        "        credentials = ee.ServiceAccountCredentials(SERVICE_ACCOUNT, KEY_FILE)\n",
        "        ee.Initialize(credentials)\n",
        "        logger.info(' Google Earth Engine initialized successfully with service account')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f' Failed to initialize GEE: {e}')\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_date_flexible(date_str):\n",
        "    \"\"\"Flexible date parsing\"\"\"\n",
        "    if pd.isna(date_str):\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Try parsing with dateutil\n",
        "        parsed_date = parser.parse(str(date_str))\n",
        "        return parsed_date\n",
        "    except Exception as e:\n",
        "        logger.warning(f'Failed to parse date: {date_str}, error: {e}')\n",
        "        return None\n",
        "\n",
        "def get_download_year(peak_date, strategy):\n",
        "    \"\"\"Determine download year based on strategy\"\"\"\n",
        "    if pd.isna(peak_date):\n",
        "        return None\n",
        "    \n",
        "    parsed_date = parse_date_flexible(peak_date)\n",
        "    if parsed_date is None:\n",
        "        return None\n",
        "    \n",
        "    peak_year = parsed_date.year\n",
        "    \n",
        "    if strategy == 'early':\n",
        "        # Download year prior to peak date\n",
        "        download_year = peak_year - 1\n",
        "    elif strategy == 'later':\n",
        "        # Download year after peak date\n",
        "        download_year = peak_year + 1\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "    # AlphaEarth data starts from 2017\n",
        "    if download_year < 2017:\n",
        "        return None\n",
        "    \n",
        "    return download_year\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Fix: Using filterBounds()\n",
        "\n",
        "**Critical Note**: The key to successful embedding extraction is using `filterBounds(region)` before `sampleRectangle()`. This prevents the \"Too many pixels in sample\" error by:\n",
        "\n",
        "1. **First filtering** the image collection to only images that intersect with our region\n",
        "2. **Then sampling** from the filtered (smaller) image\n",
        "\n",
        "**Without filterBounds**: Direct sampling from full-year images → Too many pixels → Error\n",
        "**With filterBounds**: Filter to region first → Sample from smaller image → Success\n",
        "\n",
        "This matches the approach used in the successful `/u/wz53/alphaearth/csv_embedding_extractor.py` script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_250x250_patch(latitude, longitude, year, max_retries=3):\n",
        "    \"\"\"Extract 250x250 pixel patch from AlphaEarth data with improved error handling\"\"\"\n",
        "    \n",
        "    # Validate coordinates\n",
        "    if not (-90 <= latitude <= 90) or not (-180 <= longitude <= 180):\n",
        "        logger.error(f'Invalid coordinates: lat={latitude}, lon={longitude}')\n",
        "        return None\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            logger.info(f'Attempt {attempt + 1}/{max_retries} for ({latitude:.4f}, {longitude:.4f}) in {year}')\n",
        "            \n",
        "            point = ee.Geometry.Point([longitude, latitude])\n",
        "            \n",
        "            # Try different region sizes if the first attempt fails\n",
        "            if attempt == 0:\n",
        "                # Standard 250x250 pixels (2500m x 2500m)\n",
        "                half_size_meters = 1250\n",
        "            elif attempt == 1:\n",
        "                # Smaller 128x128 pixels (1280m x 1280m)\n",
        "                half_size_meters = 640\n",
        "            else:\n",
        "                # Even smaller 64x64 pixels (640m x 640m)\n",
        "                half_size_meters = 320\n",
        "            \n",
        "            lat_rad = np.radians(latitude)\n",
        "            meters_per_deg_lat = 111320\n",
        "            meters_per_deg_lon = 111320 * np.cos(lat_rad)\n",
        "            \n",
        "            half_size_lat = half_size_meters / meters_per_deg_lat\n",
        "            half_size_lon = half_size_meters / meters_per_deg_lon\n",
        "            \n",
        "            west = longitude - half_size_lon\n",
        "            east = longitude + half_size_lon\n",
        "            south = latitude - half_size_lat\n",
        "            north = latitude + half_size_lat\n",
        "            \n",
        "            region = ee.Geometry.Rectangle([west, south, east, north])\n",
        "            \n",
        "            # Load AlphaEarth dataset and filter by bounds first\n",
        "            embedding_collection = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n",
        "            filtered_collection = embedding_collection.filterBounds(region).filterDate(\n",
        "                f'{year}-01-01', f'{year+1}-01-01'\n",
        "            )\n",
        "            \n",
        "            count = filtered_collection.size().getInfo()\n",
        "            if count == 0:\n",
        "                logger.warning(f'No AlphaEarth data found for year {year} at ({latitude:.4f}, {longitude:.4f})')\n",
        "                return None\n",
        "            \n",
        "            logger.info(f'Found {count} images for year {year}')\n",
        "            \n",
        "            # Get the first image from the filtered collection\n",
        "            image = filtered_collection.first()\n",
        "            \n",
        "            # Sample the image using sampleRectangle with timeout\n",
        "            pixel_data = image.sampleRectangle(\n",
        "                region=region,\n",
        "                defaultValue=0,\n",
        "                properties=[]\n",
        "            )\n",
        "            \n",
        "            # Get the values with timeout\n",
        "            pixel_dict = pixel_data.getInfo()\n",
        "            if not pixel_dict or 'properties' not in pixel_dict:\n",
        "                logger.warning(f'No data found for point ({latitude:.4f}, {longitude:.4f}) in year {year}')\n",
        "                if attempt < max_retries - 1:\n",
        "                    logger.info(f'Retrying with smaller region...')\n",
        "                    continue\n",
        "                return None\n",
        "            \n",
        "            # Extract embedding bands data\n",
        "            properties = pixel_dict['properties']\n",
        "            bands_data = {}\n",
        "            for i in range(64):\n",
        "                band_name = f'A{i:02d}'\n",
        "                if band_name in properties:\n",
        "                    band_array = np.array(properties[band_name])\n",
        "                    # Apply flipud for correct display\n",
        "                    band_array = np.flipud(band_array)\n",
        "                    bands_data[band_name] = band_array\n",
        "            \n",
        "            if len(bands_data) == 0:\n",
        "                logger.warning(f'No embedding bands found for point ({latitude:.4f}, {longitude:.4f}) in year {year}')\n",
        "                if attempt < max_retries - 1:\n",
        "                    logger.info(f'Retrying with smaller region...')\n",
        "                    continue\n",
        "                return None\n",
        "            \n",
        "            logger.info(f'Successfully extracted {len(bands_data)} bands')\n",
        "            \n",
        "            # Stack all 64 bands into a 64×H×W array\n",
        "            band_names = [f'A{i:02d}' for i in range(64)]\n",
        "            image_stack = []\n",
        "            \n",
        "            for band_name in band_names:\n",
        "                if band_name in bands_data:\n",
        "                    image_stack.append(bands_data[band_name])\n",
        "                else:\n",
        "                    # Fill missing bands with zeros\n",
        "                    if bands_data:\n",
        "                        image_shape = list(bands_data.values())[0].shape\n",
        "                        image_stack.append(np.zeros(image_shape))\n",
        "                    else:\n",
        "                        return None\n",
        "            \n",
        "            # Stack to create 64×H×W array\n",
        "            patch = np.stack(image_stack, axis=0)\n",
        "            \n",
        "            logger.info(f'Successfully created patch with shape: {patch.shape}')\n",
        "            return patch\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            logger.error(f'Attempt {attempt + 1} failed for ({latitude:.4f}, {longitude:.4f}) in {year}: {error_msg}')\n",
        "            \n",
        "            # Check for specific error types\n",
        "            if \"Too many pixels\" in error_msg:\n",
        "                logger.info(f'Too many pixels error - will try smaller region on next attempt')\n",
        "            elif \"timeout\" in error_msg.lower():\n",
        "                logger.info(f'Timeout error - will retry')\n",
        "            elif \"quota\" in error_msg.lower():\n",
        "                logger.warning(f'Quota exceeded - waiting before retry')\n",
        "                time.sleep(5)\n",
        "            \n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)  # Wait before retry\n",
        "                continue\n",
        "            else:\n",
        "                logger.error(f'All {max_retries} attempts failed for ({latitude:.4f}, {longitude:.4f}) in {year}')\n",
        "                return None\n",
        "    \n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_patch_as_numpy(patch, output_path, feature_id, latitude, longitude, year):\n",
        "    \"\"\"Save patch as compressed numpy file with metadata (matching original format)\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        \n",
        "        # Create band names array (matching original format)\n",
        "        band_names = [f'A{i:02d}' for i in range(64)]\n",
        "        \n",
        "        np.savez_compressed(\n",
        "            output_path,\n",
        "            image_data=patch,  # 64×H×W array\n",
        "            feature_id=feature_id,\n",
        "            centroid_lon=longitude,  # Note: order should be lon, lat (matching original)\n",
        "            centroid_lat=latitude,\n",
        "            year=year,\n",
        "            num_images=1,\n",
        "            band_names=band_names,\n",
        "            flipud_applied=True  # 标记已应用flipud\n",
        "        )\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error saving patch to {output_path}: {e}')\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#def process_single_file(file_config, max_features=None, start_idx=0):\n",
        "# setting max_features=3, and only process the first 3 features\n",
        "def process_single_file(file_config, max_features=3, start_idx=0):\n",
        "    \"\"\"Process a single file for embedding download\"\"\"\n",
        "    logger.info(f'   Processing: {file_config[\"description\"]}')\n",
        "    logger.info(f'   File: {file_config[\"file\"]}')\n",
        "    logger.info(f'   Strategy: {file_config[\"strategy\"]}')\n",
        "    logger.info(f'   Output: {file_config[\"output_dir\"]}')\n",
        "    logger.info(f'   Prefix: {file_config[\"prefix\"]}')\n",
        "    logger.info(f'   Max features: {max_features}')\n",
        "    \n",
        "    # Read CSV file\n",
        "    try:\n",
        "        df = pd.read_csv(file_config['file'])\n",
        "        logger.info(f'📊 Records: {len(df):,}')\n",
        "    except Exception as e:\n",
        "        logger.error(f' Failed to read file: {e}')\n",
        "        return {'successful': 0, 'failed': 0, 'skipped': 0}\n",
        "    \n",
        "    # Prepare features for download\n",
        "    features = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if max_features and len(features) >= max_features:\n",
        "            break\n",
        "        \n",
        "        if idx < start_idx:\n",
        "            continue\n",
        "        \n",
        "        # Get coordinates\n",
        "        lat = row.get('latitude')\n",
        "        lon = row.get('longitude')\n",
        "        \n",
        "        if pd.isna(lat) or pd.isna(lon):\n",
        "            continue\n",
        "        \n",
        "        # Get date field based on file type\n",
        "        if 'Peak_Data' in file_config['file']:\n",
        "            date_field = 'peak_date'\n",
        "        else:\n",
        "            date_field = 'matched_peak_date'\n",
        "        \n",
        "        peak_date = row.get(date_field)\n",
        "        download_year = get_download_year(peak_date, file_config['strategy'])\n",
        "        \n",
        "        if download_year is None:\n",
        "            continue\n",
        "        \n",
        "        # Get feature ID\n",
        "        if 'ID' in row:\n",
        "            feature_id = row['ID']\n",
        "        else:\n",
        "            feature_id = idx\n",
        "        \n",
        "        # Create filename\n",
        "        filename = f'{file_config[\"prefix\"]}{feature_id}.npz'\n",
        "        output_path = os.path.join(file_config['output_dir'], filename)\n",
        "        \n",
        "        # Skip if exists\n",
        "        if os.path.exists(output_path):\n",
        "            continue\n",
        "        \n",
        "        features.append({\n",
        "            'feature_id': feature_id,\n",
        "            'latitude': lat,\n",
        "            'longitude': lon,\n",
        "            'download_year': download_year,\n",
        "            'filename': filename,\n",
        "            'output_path': output_path\n",
        "        })\n",
        "    \n",
        "    logger.info(f'📋 Valid features for download: {len(features):,}')\n",
        "    \n",
        "    if not features:\n",
        "        logger.info(' No valid features to download')\n",
        "        return {'successful': 0, 'failed': 0, 'skipped': 0}\n",
        "    \n",
        "    # Download embeddings with improved error handling\n",
        "    stats = {'successful': 0, 'failed': 0, 'skipped': 0}\n",
        "    \n",
        "    # Process features sequentially to avoid GEE quota issues\n",
        "    for i, feature in enumerate(features):\n",
        "        logger.info(f'Processing feature {i+1}/{len(features)}: {feature[\"filename\"]}')\n",
        "        \n",
        "        try:\n",
        "            # Extract patch with retry mechanism\n",
        "            patch = extract_250x250_patch(\n",
        "                feature['latitude'],\n",
        "                feature['longitude'],\n",
        "                feature['download_year'],\n",
        "                max_retries=3\n",
        "            )\n",
        "            \n",
        "            if patch is not None:\n",
        "                # Save patch\n",
        "                if save_patch_as_numpy(patch, feature['output_path'], feature['feature_id'], \n",
        "                                   feature['latitude'], feature['longitude'], feature['download_year']):\n",
        "                    stats['successful'] += 1\n",
        "                    logger.info(f' Successfully saved: {feature[\"filename\"]}')\n",
        "                else:\n",
        "                    stats['failed'] += 1\n",
        "                    logger.error(f' Failed to save: {feature[\"filename\"]}')\n",
        "            else:\n",
        "                stats['failed'] += 1\n",
        "                logger.error(f' Failed to extract patch: {feature[\"filename\"]}')\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f' Error processing {feature[\"filename\"]}: {e}')\n",
        "            stats['failed'] += 1\n",
        "        \n",
        "        # Rate limiting between features to avoid quota issues\n",
        "        time.sleep(1)\n",
        "    \n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_embedding_availability(latitude, longitude, year):\n",
        "    \"\"\"Diagnose AlphaEarth data availability for a specific location and year\"\"\"\n",
        "    try:\n",
        "        logger.info(f' Diagnosing AlphaEarth data for ({latitude:.4f}, {longitude:.4f}) in {year}')\n",
        "        \n",
        "        # Create a small test region\n",
        "        point = ee.Geometry.Point([longitude, latitude])\n",
        "        half_size_meters = 100  # Small 200m x 200m region for testing\n",
        "        lat_rad = np.radians(latitude)\n",
        "        meters_per_deg_lat = 111320\n",
        "        meters_per_deg_lon = 111320 * np.cos(lat_rad)\n",
        "        \n",
        "        half_size_lat = half_size_meters / meters_per_deg_lat\n",
        "        half_size_lon = half_size_meters / meters_per_deg_lon\n",
        "        \n",
        "        west = longitude - half_size_lon\n",
        "        east = longitude + half_size_lon\n",
        "        south = latitude - half_size_lat\n",
        "        north = latitude + half_size_lat\n",
        "        \n",
        "        region = ee.Geometry.Rectangle([west, south, east, north])\n",
        "        \n",
        "        # Check AlphaEarth collection\n",
        "        embedding_collection = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n",
        "        \n",
        "        # Check total collection size\n",
        "        total_size = embedding_collection.size().getInfo()\n",
        "        logger.info(f'Total AlphaEarth images: {total_size}')\n",
        "        \n",
        "        # Check filtered collection\n",
        "        filtered_collection = embedding_collection.filterBounds(region).filterDate(\n",
        "            f'{year}-01-01', f'{year+1}-01-01'\n",
        "        )\n",
        "        \n",
        "        filtered_size = filtered_collection.size().getInfo()\n",
        "        logger.info(f'Filtered images for {year}: {filtered_size}')\n",
        "        \n",
        "        if filtered_size > 0:\n",
        "            # Get image info\n",
        "            image = filtered_collection.first()\n",
        "            image_info = image.getInfo()\n",
        "            logger.info(f'Image properties: {list(image_info.keys())}')\n",
        "            \n",
        "            # Check band names\n",
        "            if 'bands' in image_info:\n",
        "                band_names = [band['id'] for band in image_info['bands']]\n",
        "                logger.info(f'Available bands: {band_names[:10]}... (showing first 10)')\n",
        "            \n",
        "            return True\n",
        "        else:\n",
        "            logger.warning(f'No AlphaEarth data available for {year}')\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f'Diagnosis failed: {e}')\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 01:32:20,734 - INFO -  Diagnosing AlphaEarth data for (42.9575, -91.6240) in 2017\n",
            "2025-10-21 01:32:20,734 - ERROR - Diagnosis failed: Earth Engine client library not initialized. See http://goo.gle/ee-auth.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Testing AlphaEarth data availability for first feature...\n",
            "First feature: lat=42.95753, lon=-91.62403, peak_date=2016-08-25 12:00:00, download_year=2017\n"
          ]
        }
      ],
      "source": [
        "# Test diagnosis for the first feature\n",
        "print(' Testing AlphaEarth data availability for first feature...')\n",
        "\n",
        "# Read the first CSV file to get coordinates\n",
        "test_file = '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv'\n",
        "df = pd.read_csv(test_file)\n",
        "\n",
        "# Get first row\n",
        "first_row = df.iloc[0]\n",
        "lat = first_row['latitude']\n",
        "lon = first_row['longitude']\n",
        "peak_date = first_row['peak_date']\n",
        "\n",
        "# Calculate download year\n",
        "download_year = get_download_year(peak_date, 'later')\n",
        "print(f'First feature: lat={lat}, lon={lon}, peak_date={peak_date}, download_year={download_year}')\n",
        "\n",
        "# Diagnose availability\n",
        "if download_year:\n",
        "    diagnose_embedding_availability(lat, lon, download_year)\n",
        "else:\n",
        "    print(' Could not determine download year')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 01:32:21,149 - INFO -  Google Earth Engine initialized successfully with service account\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GEE initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize GEE\n",
        "if not initialize_gee():\n",
        "    raise Exception(\"Failed to initialize Google Earth Engine\")\n",
        "\n",
        "print(' GEE initialized successfully!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Created directory: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early\n",
            " Created directory: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later\n"
          ]
        }
      ],
      "source": [
        "# Create output directories\n",
        "output_dirs = [\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early',\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later'\n",
        "]\n",
        "\n",
        "for output_dir in output_dirs:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f' Created directory: {output_dir}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 File configurations defined:\n",
            "   1. 2016-2017 Peak Data (later strategy)\n",
            "      File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv\n",
            "      Strategy: later\n",
            "      Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later_example\n",
            "      Prefix: gage_\n",
            "\n",
            "   2. 2018+ Peak Data (early strategy)\n",
            "      File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\n",
            "      Strategy: early\n",
            "      Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early_example\n",
            "      Prefix: gage_\n",
            "\n",
            "   3. 2016-2017 Matched Records (later strategy)\n",
            "      File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv\n",
            "      Strategy: later\n",
            "      Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later_example\n",
            "      Prefix: HWM_\n",
            "\n",
            "   4. 2018+ Matched Records (early strategy)\n",
            "      File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv\n",
            "      Strategy: early\n",
            "      Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early_example\n",
            "      Prefix: HWM_\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define file configurations\n",
        "file_configs = [\n",
        "    {\n",
        "        'description': '2016-2017 Peak Data (later strategy)',\n",
        "        'file': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv',\n",
        "        'strategy': 'later',\n",
        "        'output_dir': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later_example',\n",
        "        'prefix': 'gage_'\n",
        "    },\n",
        "    {\n",
        "        'description': '2018+ Peak Data (early strategy)',\n",
        "        'file': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv',\n",
        "        'strategy': 'early',\n",
        "        'output_dir': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early_example',\n",
        "        'prefix': 'gage_'\n",
        "    },\n",
        "    {\n",
        "        'description': '2016-2017 Matched Records (later strategy)',\n",
        "        'file': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv',\n",
        "        'strategy': 'later',\n",
        "        'output_dir': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later_example',\n",
        "        'prefix': 'HWM_'\n",
        "    },\n",
        "    {\n",
        "        'description': '2018+ Matched Records (early strategy)',\n",
        "        'file': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv',\n",
        "        'strategy': 'early',\n",
        "        'output_dir': '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early_example',\n",
        "        'prefix': 'HWM_'\n",
        "    }\n",
        "]\n",
        "\n",
        "print('📋 File configurations defined:')\n",
        "for i, config in enumerate(file_configs, 1):\n",
        "    print(f'   {i}. {config[\"description\"]}')\n",
        "    print(f'      File: {config[\"file\"]}')\n",
        "    print(f'      Strategy: {config[\"strategy\"]}')\n",
        "    print(f'      Output: {config[\"output_dir\"]}')\n",
        "    print(f'      Prefix: {config[\"prefix\"]}')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 01:32:21,266 - INFO -    Processing: 2016-2017 Peak Data (later strategy)\n",
            "2025-10-21 01:32:21,267 - INFO -    File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv\n",
            "2025-10-21 01:32:21,267 - INFO -    Strategy: later\n",
            "2025-10-21 01:32:21,267 - INFO -    Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later_example\n",
            "2025-10-21 01:32:21,267 - INFO -    Prefix: gage_\n",
            "2025-10-21 01:32:21,267 - INFO -    Max features: 3\n",
            "2025-10-21 01:32:21,279 - INFO - 📊 Records: 1,949\n",
            "2025-10-21 01:32:21,291 - INFO - 📋 Valid features for download: 3\n",
            "2025-10-21 01:32:21,291 - INFO - Processing feature 1/3: gage_10007.npz\n",
            "2025-10-21 01:32:21,291 - INFO - Attempt 1/3 for (43.0391, -91.7701) in 2017\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Multi-File AlphaEarth Embedding Download (Limited to 3 files, 3 features each)\n",
            "======================================================================\n",
            "\n",
            " Processing file 1/4: 2016-2017 Peak Data (later strategy)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 01:32:21,408 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:32:27,785 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:32:27,794 - INFO - Successfully created patch with shape: (64, 254, 255)\n",
            "2025-10-21 01:32:28,623 - INFO -  Successfully saved: gage_10007.npz\n",
            "2025-10-21 01:32:29,625 - INFO - Processing feature 2/3: gage_10008.npz\n",
            "2025-10-21 01:32:29,626 - INFO - Attempt 1/3 for (43.0393, -91.7704) in 2017\n",
            "2025-10-21 01:32:29,747 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:32:38,250 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:32:38,258 - INFO - Successfully created patch with shape: (64, 254, 255)\n",
            "2025-10-21 01:32:39,096 - INFO -  Successfully saved: gage_10008.npz\n",
            "2025-10-21 01:32:40,099 - INFO - Processing feature 3/3: gage_10009.npz\n",
            "2025-10-21 01:32:40,099 - INFO - Attempt 1/3 for (43.0544, -91.8083) in 2017\n",
            "2025-10-21 01:32:40,220 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:32:49,869 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:32:49,878 - INFO - Successfully created patch with shape: (64, 254, 255)\n",
            "2025-10-21 01:32:50,745 - INFO -  Successfully saved: gage_10009.npz\n",
            "2025-10-21 01:32:51,748 - INFO -    Processing: 2018+ Peak Data (early strategy)\n",
            "2025-10-21 01:32:51,749 - INFO -    File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\n",
            "2025-10-21 01:32:51,749 - INFO -    Strategy: early\n",
            "2025-10-21 01:32:51,749 - INFO -    Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early_example\n",
            "2025-10-21 01:32:51,749 - INFO -    Prefix: gage_\n",
            "2025-10-21 01:32:51,749 - INFO -    Max features: 3\n",
            "2025-10-21 01:32:51,761 - INFO - 📊 Records: 2,297\n",
            "2025-10-21 01:32:51,770 - INFO - 📋 Valid features for download: 3\n",
            "2025-10-21 01:32:51,770 - INFO - Processing feature 1/3: gage_12008.npz\n",
            "2025-10-21 01:32:51,770 - INFO - Attempt 1/3 for (41.2867, -72.6461) in 2017\n",
            "2025-10-21 01:32:51,910 - INFO - Found 1 images for year 2017\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Results: 3 successful, 0 failed\n",
            "\n",
            " Processing file 2/4: 2018+ Peak Data (early strategy)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 01:32:56,995 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:32:57,005 - INFO - Successfully created patch with shape: (64, 257, 259)\n",
            "2025-10-21 01:32:57,952 - INFO -  Successfully saved: gage_12008.npz\n",
            "2025-10-21 01:32:58,955 - INFO - Processing feature 2/3: gage_12009.npz\n",
            "2025-10-21 01:32:58,955 - INFO - Attempt 1/3 for (41.3444, -71.9095) in 2017\n",
            "2025-10-21 01:32:59,085 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:33:10,236 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:33:10,245 - INFO - Successfully created patch with shape: (64, 259, 260)\n",
            "2025-10-21 01:33:11,243 - INFO -  Successfully saved: gage_12009.npz\n",
            "2025-10-21 01:33:12,245 - INFO - Processing feature 3/3: gage_12010.npz\n",
            "2025-10-21 01:33:12,245 - INFO - Attempt 1/3 for (42.0496, -70.1819) in 2017\n",
            "2025-10-21 01:33:12,414 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:33:23,558 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:33:23,566 - INFO - Successfully created patch with shape: (64, 254, 255)\n",
            "2025-10-21 01:33:24,321 - INFO -  Successfully saved: gage_12010.npz\n",
            "2025-10-21 01:33:25,325 - INFO -    Processing: 2016-2017 Matched Records (later strategy)\n",
            "2025-10-21 01:33:25,326 - INFO -    File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv\n",
            "2025-10-21 01:33:25,326 - INFO -    Strategy: later\n",
            "2025-10-21 01:33:25,326 - INFO -    Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later_example\n",
            "2025-10-21 01:33:25,326 - INFO -    Prefix: HWM_\n",
            "2025-10-21 01:33:25,327 - INFO -    Max features: 3\n",
            "2025-10-21 01:33:25,344 - INFO - 📊 Records: 1,947\n",
            "2025-10-21 01:33:25,352 - INFO - 📋 Valid features for download: 3\n",
            "2025-10-21 01:33:25,352 - INFO - Processing feature 1/3: HWM_7.npz\n",
            "2025-10-21 01:33:25,352 - INFO - Attempt 1/3 for (43.0178, -91.7310) in 2017\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Results: 3 successful, 0 failed\n",
            "\n",
            " Processing file 3/4: 2016-2017 Matched Records (later strategy)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 01:33:25,612 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:33:35,674 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:33:35,681 - INFO - Successfully created patch with shape: (64, 254, 255)\n",
            "2025-10-21 01:33:36,563 - INFO -  Successfully saved: HWM_7.npz\n",
            "2025-10-21 01:33:37,566 - INFO - Processing feature 2/3: HWM_8.npz\n",
            "2025-10-21 01:33:37,566 - INFO - Attempt 1/3 for (43.0180, -91.7306) in 2017\n",
            "2025-10-21 01:33:37,675 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:33:44,354 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:33:44,363 - INFO - Successfully created patch with shape: (64, 254, 255)\n",
            "2025-10-21 01:33:45,239 - INFO -  Successfully saved: HWM_8.npz\n",
            "2025-10-21 01:33:46,241 - INFO - Processing feature 3/3: HWM_9.npz\n",
            "2025-10-21 01:33:46,241 - INFO - Attempt 1/3 for (43.0513, -91.8411) in 2017\n",
            "2025-10-21 01:33:46,362 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:33:54,341 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:33:54,348 - INFO - Successfully created patch with shape: (64, 254, 254)\n",
            "2025-10-21 01:33:55,231 - INFO -  Successfully saved: HWM_9.npz\n",
            "2025-10-21 01:33:56,236 - INFO -    Processing: 2018+ Matched Records (early strategy)\n",
            "2025-10-21 01:33:56,237 - INFO -    File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv\n",
            "2025-10-21 01:33:56,237 - INFO -    Strategy: early\n",
            "2025-10-21 01:33:56,237 - INFO -    Output: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early_example\n",
            "2025-10-21 01:33:56,237 - INFO -    Prefix: HWM_\n",
            "2025-10-21 01:33:56,237 - INFO -    Max features: 3\n",
            "2025-10-21 01:33:56,248 - INFO - 📊 Records: 698\n",
            "2025-10-21 01:33:56,258 - INFO - 📋 Valid features for download: 3\n",
            "2025-10-21 01:33:56,258 - INFO - Processing feature 1/3: HWM_1956.npz\n",
            "2025-10-21 01:33:56,259 - INFO - Attempt 1/3 for (33.5547, -79.0342) in 2017\n",
            "2025-10-21 01:33:56,398 - INFO - Found 1 images for year 2017\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Results: 3 successful, 0 failed\n",
            "\n",
            " Processing file 4/4: 2018+ Matched Records (early strategy)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 01:34:06,074 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:34:06,083 - INFO - Successfully created patch with shape: (64, 255, 256)\n",
            "2025-10-21 01:34:06,993 - INFO -  Successfully saved: HWM_1956.npz\n",
            "2025-10-21 01:34:07,995 - INFO - Processing feature 2/3: HWM_1963.npz\n",
            "2025-10-21 01:34:07,995 - INFO - Attempt 1/3 for (34.3100, -79.8448) in 2017\n",
            "2025-10-21 01:34:08,110 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:34:17,162 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:34:17,172 - INFO - Successfully created patch with shape: (64, 253, 254)\n",
            "2025-10-21 01:34:18,098 - INFO -  Successfully saved: HWM_1963.npz\n",
            "2025-10-21 01:34:19,099 - INFO - Processing feature 3/3: HWM_1964.npz\n",
            "2025-10-21 01:34:19,100 - INFO - Attempt 1/3 for (35.0658, -76.9673) in 2017\n",
            "2025-10-21 01:34:19,224 - INFO - Found 1 images for year 2017\n",
            "2025-10-21 01:34:28,225 - INFO - Successfully extracted 64 bands\n",
            "2025-10-21 01:34:28,234 - INFO - Successfully created patch with shape: (64, 255, 256)\n",
            "2025-10-21 01:34:29,121 - INFO -  Successfully saved: HWM_1964.npz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Results: 3 successful, 0 failed\n",
            "\n",
            " All embedding downloads completed!\n",
            " Final Summary:\n",
            "   Total successful: 12\n",
            "   Total failed: 0\n",
            "   Total skipped (existing): 0\n",
            "   Overall success rate: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Start downloading embeddings\n",
        "#print('Starting Multi-File AlphaEarth Embedding Download')\n",
        "#print('=' * 50)\n",
        "# Start downloading embeddings (only first 3 files, 3 features each)\n",
        "print('Starting Multi-File AlphaEarth Embedding Download (Limited to 3 files, 3 features each)')\n",
        "print('=' * 70)\n",
        "\n",
        "total_stats = {'successful': 0, 'failed': 0, 'skipped': 0}\n",
        "\n",
        "for i, file_config in enumerate(file_configs, 1):\n",
        "    print(f'\\n Processing file {i}/4: {file_config[\"description\"]}')\n",
        "    \n",
        "    # Process file\n",
        "    #stats = process_single_file(file_config)\n",
        "    # Process file (max 3 features per file)\n",
        "    stats = process_single_file(file_config, max_features=3)    \n",
        "    # Update totals\n",
        "    for key in total_stats:\n",
        "        total_stats[key] += stats[key]\n",
        "    \n",
        "    print(f' Results: {stats[\"successful\"]} successful, {stats[\"failed\"]} failed')\n",
        "\n",
        "print(f'\\n All embedding downloads completed!')\n",
        "print(f' Final Summary:')\n",
        "print(f'   Total successful: {total_stats[\"successful\"]:,}')\n",
        "print(f'   Total failed: {total_stats[\"failed\"]:,}')\n",
        "print(f'   Total skipped (existing): {total_stats[\"skipped\"]:,}')\n",
        "\n",
        "# Calculate success rate with zero division protection\n",
        "total_attempted = total_stats[\"successful\"] + total_stats[\"failed\"]\n",
        "if total_attempted > 0:\n",
        "    success_rate = total_stats[\"successful\"] / total_attempted * 100\n",
        "    print(f'   Overall success rate: {success_rate:.1f}%')\n",
        "else:\n",
        "    print(f'   Overall success rate: N/A (no downloads attempted)')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checking downloaded files:\n",
            "   /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early: 2,995 files\n",
            "   /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later: 3,896 files\n"
          ]
        }
      ],
      "source": [
        "# Check downloaded files\n",
        "print('\\n  Checking downloaded files:')\n",
        "\n",
        "for output_dir in output_dirs:\n",
        "    if os.path.exists(output_dir):\n",
        "        files = [f for f in os.listdir(output_dir) if f.endswith('.npz')]\n",
        "        print(f'   {output_dir}: {len(files):,} files')\n",
        "    else:\n",
        "        print(f'   {output_dir}: Directory not found')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Verifying file format (sample check):\n",
            "\n",
            "📁 File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early/HWM_1949.npz\n",
            "   Keys: ['image_data', 'feature_id', 'centroid_lon', 'centroid_lat', 'year', 'num_images', 'band_names', 'flipud_applied']\n",
            "   Image shape: (64, 256, 257)\n",
            "   Data type: float64\n",
            "   Feature ID: 1949\n",
            "   Year: 2017\n",
            "   Band names: ['A00' 'A01' 'A02' 'A03' 'A04']... (first 5)\n",
            "\n",
            "📁 File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later/HWM_1.npz\n",
            "   Keys: ['image_data', 'feature_id', 'centroid_lon', 'centroid_lat', 'year', 'num_images', 'band_names', 'flipud_applied']\n",
            "   Image shape: (64, 255, 256)\n",
            "   Data type: float64\n",
            "   Feature ID: 1\n",
            "   Year: 2017\n",
            "   Band names: ['A00' 'A01' 'A02' 'A03' 'A04']... (first 5)\n"
          ]
        }
      ],
      "source": [
        "# Verify file format (sample check)\n",
        "print('\\n🔍 Verifying file format (sample check):')\n",
        "\n",
        "sample_files = []\n",
        "for output_dir in output_dirs:\n",
        "    if os.path.exists(output_dir):\n",
        "        files = [f for f in os.listdir(output_dir) if f.endswith('.npz')]\n",
        "        if files:\n",
        "            sample_files.append(os.path.join(output_dir, files[0]))\n",
        "\n",
        "for file_path in sample_files[:2]:  # Check first 2 files\n",
        "    print(f'\\n📁 File: {file_path}')\n",
        "    try:\n",
        "        data = np.load(file_path)\n",
        "        print(f'   Keys: {list(data.keys())}')\n",
        "        if 'image_data' in data:\n",
        "            print(f'   Image shape: {data[\"image_data\"].shape}')\n",
        "            print(f'   Data type: {data[\"image_data\"].dtype}')\n",
        "        if 'feature_id' in data:\n",
        "            print(f'   Feature ID: {data[\"feature_id\"]}')\n",
        "        if 'year' in data:\n",
        "            print(f'   Year: {data[\"year\"]}')\n",
        "        if 'band_names' in data:\n",
        "            print(f'   Band names: {data[\"band_names\"][:5]}... (first 5)')\n",
        "    except Exception as e:\n",
        "        print(f'   Error: {e}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
