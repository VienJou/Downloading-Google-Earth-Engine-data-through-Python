{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-File AlphaEarth Embedding Download Notebook (Improved Version)\n",
    "\n",
    "This notebook downloads AlphaEarth embeddings for **only the first 3 CSV files** with different temporal strategies, processing **only the first 3 features** from each file:\n",
    "\n",
    "## Files and Strategies (Limited to 3 files):\n",
    "\n",
    "1. **Unified_Peak_Data_2016_2017_with_ID(1006).csv** → embedding_1Y_later (download 2017, 2018) - **3 samples only**\n",
    "2. **Unified_Peak_Data_2018_and_later_with_ID(1006).csv** → embedding_1Y_early (download previous year) - **3 samples only**\n",
    "3. **matched_records_1947_with_ID_2016_2017(1006).csv** → embedding_1Y_later (download 2017, 2018) - **3 samples only**\n",
    "\n",
    "## Improvements:\n",
    "- **Adaptive region sizing**: Starts with 250x250 pixels, reduces to 128x128 or 64x64 if needed\n",
    "- **Retry mechanism**: Up to 3 attempts per feature with different strategies\n",
    "- **Better error handling**: Detailed logging and specific error type detection\n",
    "- **Sequential processing**: Avoids GEE quota issues\n",
    "- **Data availability diagnosis**: Tests AlphaEarth data availability before processing\n",
    "\n",
    "## Data Format:\n",
    "- **Structure**: 64×H×W arrays (64 bands A00-A63)\n",
    "- **Naming**: gage_[ID] for Unified Peak Data, HWM_[ID] for matched records\n",
    "- **Output**: Compressed .npz files with metadata\n",
    "- **Limit**: Maximum 3 features per file (9 total features)\n",
    "\n",
    "## Authentication:\n",
    "Uses service account: `zhouwenlc@windy-winter-456502-b1.iam.gserviceaccount.com`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: earthengine-api in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (1.1.5)\n",
      "Requirement already satisfied: google-cloud-storage in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from earthengine-api) (3.4.1)\n",
      "Requirement already satisfied: google-api-python-client>=1.12.1 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from earthengine-api) (2.89.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from earthengine-api) (2.41.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from earthengine-api) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from earthengine-api) (0.22.0)\n",
      "Requirement already satisfied: requests in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from earthengine-api) (2.31.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->earthengine-api) (2.26.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->earthengine-api) (4.1.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from google-auth>=1.4.1->earthengine-api) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from google-auth>=1.4.1->earthengine-api) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from google-auth>=1.4.1->earthengine-api) (4.9)\n",
      "Requirement already satisfied: six in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from google-auth-httplib2>=0.0.3->earthengine-api) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from httplib2<1dev,>=0.9.2->earthengine-api) (3.1.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from google-cloud-storage->earthengine-api) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from google-cloud-storage->earthengine-api) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from google-cloud-storage->earthengine-api) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from requests->earthengine-api) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from requests->earthengine-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from requests->earthengine-api) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from requests->earthengine-api) (2023.7.22)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.57.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (4.25.8)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/jovyan/.local/python3-0.9.4/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.26.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /cvmfs/cybergis.illinois.edu/software/conda/cybergisx/python3-0.9.4/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install earthengine-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dateutil import parser\n",
    "import time\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gee():\n",
    "    \"\"\"Initialize Google Earth Engine using service account\"\"\"\n",
    "    try:\n",
    "        SERVICE_ACCOUNT = 'zhouwenlc@windy-winter-456502-b1.iam.gserviceaccount.com'\n",
    "        KEY_FILE = 'Flood_dataset/windy-winter-456502-b1-e3f770db867c.json'\n",
    "        credentials = ee.ServiceAccountCredentials(SERVICE_ACCOUNT, KEY_FILE)\n",
    "        ee.Initialize(credentials)\n",
    "        logger.info('Google Earth Engine initialized successfully with service account')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to initialize GEE: {e}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"Flexible date parsing\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try parsing with dateutil\n",
    "        parsed_date = parser.parse(str(date_str))\n",
    "        return parsed_date\n",
    "    except Exception as e:\n",
    "        logger.warning(f'Failed to parse date: {date_str}, error: {e}')\n",
    "        return None\n",
    "\n",
    "def get_download_year(peak_date, strategy):\n",
    "    \"\"\"Determine download year based on strategy\"\"\"\n",
    "    if pd.isna(peak_date):\n",
    "        return None\n",
    "    \n",
    "    parsed_date = parse_date_flexible(peak_date)\n",
    "    if parsed_date is None:\n",
    "        return None\n",
    "    \n",
    "    peak_year = parsed_date.year\n",
    "    \n",
    "    if strategy == 'early':\n",
    "        # Download year prior to peak date\n",
    "        download_year = peak_year - 1\n",
    "    elif strategy == 'later':\n",
    "        # Download year after peak date\n",
    "        download_year = peak_year + 1\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # AlphaEarth data starts from 2017\n",
    "    if download_year < 2017:\n",
    "        return None\n",
    "    \n",
    "    return download_year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Fix: Using filterBounds()\n",
    "\n",
    "**Critical Note**: The key to successful embedding extraction is using `filterBounds(region)` before `sampleRectangle()`. This prevents the \"Too many pixels in sample\" error by:\n",
    "\n",
    "1. **First filtering** the image collection to only images that intersect with our region\n",
    "2. **Then sampling** from the filtered (smaller) image\n",
    "\n",
    "**Without filterBounds**: Direct sampling from full-year images → Too many pixels → Error\n",
    "**With filterBounds**: Filter to region first → Sample from smaller image → Success\n",
    "\n",
    "This matches the approach used in the successful `/u/wz53/alphaearth/csv_embedding_extractor.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_250x250_patch(latitude, longitude, year, max_retries=3):\n",
    "    \"\"\"Extract 250x250 pixel patch from AlphaEarth data with improved error handling\"\"\"\n",
    "    \n",
    "    # Validate coordinates\n",
    "    if not (-90 <= latitude <= 90) or not (-180 <= longitude <= 180):\n",
    "        logger.error(f'Invalid coordinates: lat={latitude}, lon={longitude}')\n",
    "        return None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f'Attempt {attempt + 1}/{max_retries} for ({latitude:.4f}, {longitude:.4f}) in {year}')\n",
    "            \n",
    "            point = ee.Geometry.Point([longitude, latitude])\n",
    "            \n",
    "            # Try different region sizes if the first attempt fails\n",
    "            if attempt == 0:\n",
    "                # Standard 250x250 pixels (2500m x 2500m)\n",
    "                half_size_meters = 1250\n",
    "            elif attempt == 1:\n",
    "                # Smaller 128x128 pixels (1280m x 1280m)\n",
    "                half_size_meters = 640\n",
    "            else:\n",
    "                # Even smaller 64x64 pixels (640m x 640m)\n",
    "                half_size_meters = 320\n",
    "            \n",
    "            lat_rad = np.radians(latitude)\n",
    "            meters_per_deg_lat = 111320\n",
    "            meters_per_deg_lon = 111320 * np.cos(lat_rad)\n",
    "            \n",
    "            half_size_lat = half_size_meters / meters_per_deg_lat\n",
    "            half_size_lon = half_size_meters / meters_per_deg_lon\n",
    "            \n",
    "            west = longitude - half_size_lon\n",
    "            east = longitude + half_size_lon\n",
    "            south = latitude - half_size_lat\n",
    "            north = latitude + half_size_lat\n",
    "            \n",
    "            region = ee.Geometry.Rectangle([west, south, east, north])\n",
    "            \n",
    "            # Load AlphaEarth dataset and filter by bounds first\n",
    "            embedding_collection = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n",
    "            filtered_collection = embedding_collection.filterBounds(region).filterDate(\n",
    "                f'{year}-01-01', f'{year+1}-01-01'\n",
    "            )\n",
    "            \n",
    "            count = filtered_collection.size().getInfo()\n",
    "            if count == 0:\n",
    "                logger.warning(f'No AlphaEarth data found for year {year} at ({latitude:.4f}, {longitude:.4f})')\n",
    "                return None\n",
    "            \n",
    "            logger.info(f'Found {count} images for year {year}')\n",
    "            \n",
    "            # Get the first image from the filtered collection\n",
    "            image = filtered_collection.first()\n",
    "            \n",
    "            # Sample the image using sampleRectangle with timeout\n",
    "            pixel_data = image.sampleRectangle(\n",
    "                region=region,\n",
    "                defaultValue=0,\n",
    "                properties=[]\n",
    "            )\n",
    "            \n",
    "            # Get the values with timeout\n",
    "            pixel_dict = pixel_data.getInfo()\n",
    "            if not pixel_dict or 'properties' not in pixel_dict:\n",
    "                logger.warning(f'No data found for point ({latitude:.4f}, {longitude:.4f}) in year {year}')\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.info(f'Retrying with smaller region...')\n",
    "                    continue\n",
    "                return None\n",
    "            \n",
    "            # Extract embedding bands data\n",
    "            properties = pixel_dict['properties']\n",
    "            bands_data = {}\n",
    "            for i in range(64):\n",
    "                band_name = f'A{i:02d}'\n",
    "                if band_name in properties:\n",
    "                    band_array = np.array(properties[band_name])\n",
    "                    # Apply flipud for correct display\n",
    "                    band_array = np.flipud(band_array)\n",
    "                    bands_data[band_name] = band_array\n",
    "            \n",
    "            if len(bands_data) == 0:\n",
    "                logger.warning(f'No embedding bands found for point ({latitude:.4f}, {longitude:.4f}) in year {year}')\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.info(f'Retrying with smaller region...')\n",
    "                    continue\n",
    "                return None\n",
    "            \n",
    "            logger.info(f'Successfully extracted {len(bands_data)} bands')\n",
    "            \n",
    "            # Stack all 64 bands into a 64×H×W array\n",
    "            band_names = [f'A{i:02d}' for i in range(64)]\n",
    "            image_stack = []\n",
    "            \n",
    "            for band_name in band_names:\n",
    "                if band_name in bands_data:\n",
    "                    image_stack.append(bands_data[band_name])\n",
    "                else:\n",
    "                    # Fill missing bands with zeros\n",
    "                    if bands_data:\n",
    "                        image_shape = list(bands_data.values())[0].shape\n",
    "                        image_stack.append(np.zeros(image_shape))\n",
    "                    else:\n",
    "                        return None\n",
    "            \n",
    "            # Stack to create 64×H×W array\n",
    "            patch = np.stack(image_stack, axis=0)\n",
    "            \n",
    "            logger.info(f'Successfully created patch with shape: {patch.shape}')\n",
    "            return patch\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(f'Attempt {attempt + 1} failed for ({latitude:.4f}, {longitude:.4f}) in {year}: {error_msg}')\n",
    "            \n",
    "            # Check for specific error types\n",
    "            if \"Too many pixels\" in error_msg:\n",
    "                logger.info(f'Too many pixels error - will try smaller region on next attempt')\n",
    "            elif \"timeout\" in error_msg.lower():\n",
    "                logger.info(f'Timeout error - will retry')\n",
    "            elif \"quota\" in error_msg.lower():\n",
    "                logger.warning(f'Quota exceeded - waiting before retry')\n",
    "                time.sleep(5)\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)  # Wait before retry\n",
    "                continue\n",
    "            else:\n",
    "                logger.error(f'All {max_retries} attempts failed for ({latitude:.4f}, {longitude:.4f}) in {year}')\n",
    "                return None\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patch_as_numpy(patch, output_path, feature_id, latitude, longitude, year):\n",
    "    \"\"\"Save patch as compressed numpy file with metadata (matching original format)\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Create band names array (matching original format)\n",
    "        band_names = [f'A{i:02d}' for i in range(64)]\n",
    "        \n",
    "        np.savez_compressed(\n",
    "            output_path,\n",
    "            image_data=patch,  # 64×H×W array\n",
    "            feature_id=feature_id,\n",
    "            centroid_lon=longitude,  # Note: order should be lon, lat (matching original)\n",
    "            centroid_lat=latitude,\n",
    "            year=year,\n",
    "            num_images=1,\n",
    "            band_names=band_names,\n",
    "            flipud_applied=True  # 标记已应用flipud\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error saving patch to {output_path}: {e}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def process_single_file(file_config, max_features=None, start_idx=0):\n",
    "# setting max_features=3, and only process the first 3 features\n",
    "def process_single_file(file_config, max_features=3, start_idx=0):\n",
    "    \"\"\"Process a single file for embedding download\"\"\"\n",
    "    logger.info(f'   Processing: {file_config[\"description\"]}')\n",
    "    logger.info(f'   File: {file_config[\"file\"]}')\n",
    "    logger.info(f'   Strategy: {file_config[\"strategy\"]}')\n",
    "    logger.info(f'   Output: {file_config[\"output_dir\"]}')\n",
    "    logger.info(f'   Prefix: {file_config[\"prefix\"]}')\n",
    "    logger.info(f'   Max features: {max_features}')\n",
    "    \n",
    "    # Read CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(file_config['file'])\n",
    "        logger.info(f' Records: {len(df):,}')\n",
    "    except Exception as e:\n",
    "        logger.error(f' Failed to read file: {e}')\n",
    "        return {'successful': 0, 'failed': 0, 'skipped': 0}\n",
    "    \n",
    "    # Prepare features for download\n",
    "    features = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if max_features and len(features) >= max_features:\n",
    "            break\n",
    "        \n",
    "        if idx < start_idx:\n",
    "            continue\n",
    "        \n",
    "        # Get coordinates\n",
    "        lat = row.get('latitude')\n",
    "        lon = row.get('longitude')\n",
    "        \n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            continue\n",
    "        \n",
    "        # Get date field based on file type\n",
    "        if 'Peak_Data' in file_config['file']:\n",
    "            date_field = 'peak_date'\n",
    "        else:\n",
    "            date_field = 'matched_peak_date'\n",
    "        \n",
    "        peak_date = row.get(date_field)\n",
    "        download_year = get_download_year(peak_date, file_config['strategy'])\n",
    "        \n",
    "        if download_year is None:\n",
    "            continue\n",
    "        \n",
    "        # Get feature ID\n",
    "        if 'ID' in row:\n",
    "            feature_id = row['ID']\n",
    "        else:\n",
    "            feature_id = idx\n",
    "        \n",
    "        # Create filename\n",
    "        filename = f'{file_config[\"prefix\"]}{feature_id}.npz'\n",
    "        output_path = os.path.join(file_config['output_dir'], filename)\n",
    "        \n",
    "        # Skip if exists\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "        \n",
    "        features.append({\n",
    "            'feature_id': feature_id,\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'download_year': download_year,\n",
    "            'filename': filename,\n",
    "            'output_path': output_path\n",
    "        })\n",
    "    \n",
    "    logger.info(f' Valid features for download: {len(features):,}')\n",
    "    \n",
    "    if not features:\n",
    "        logger.info(' No valid features to download')\n",
    "        return {'successful': 0, 'failed': 0, 'skipped': 0}\n",
    "    \n",
    "    # Download embeddings with improved error handling\n",
    "    stats = {'successful': 0, 'failed': 0, 'skipped': 0}\n",
    "    \n",
    "    # Process features sequentially to avoid GEE quota issues\n",
    "    for i, feature in enumerate(features):\n",
    "        logger.info(f'Processing feature {i+1}/{len(features)}: {feature[\"filename\"]}')\n",
    "        \n",
    "        try:\n",
    "            # Extract patch with retry mechanism\n",
    "            patch = extract_250x250_patch(\n",
    "                feature['latitude'],\n",
    "                feature['longitude'],\n",
    "                feature['download_year'],\n",
    "                max_retries=3\n",
    "            )\n",
    "            \n",
    "            if patch is not None:\n",
    "                # Save patch\n",
    "                if save_patch_as_numpy(patch, feature['output_path'], feature['feature_id'], \n",
    "                                   feature['latitude'], feature['longitude'], feature['download_year']):\n",
    "                    stats['successful'] += 1\n",
    "                    logger.info(f' Successfully saved: {feature[\"filename\"]}')\n",
    "                else:\n",
    "                    stats['failed'] += 1\n",
    "                    logger.error(f' Failed to save: {feature[\"filename\"]}')\n",
    "            else:\n",
    "                stats['failed'] += 1\n",
    "                logger.error(f' Failed to extract patch: {feature[\"filename\"]}')\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f' Error processing {feature[\"filename\"]}: {e}')\n",
    "            stats['failed'] += 1\n",
    "        \n",
    "        # Rate limiting between features to avoid quota issues\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_embedding_availability(latitude, longitude, year):\n",
    "    \"\"\"Diagnose AlphaEarth data availability for a specific location and year\"\"\"\n",
    "    try:\n",
    "        logger.info(f' Diagnosing AlphaEarth data for ({latitude:.4f}, {longitude:.4f}) in {year}')\n",
    "        \n",
    "        # Create a small test region\n",
    "        point = ee.Geometry.Point([longitude, latitude])\n",
    "        half_size_meters = 100  # Small 200m x 200m region for testing\n",
    "        lat_rad = np.radians(latitude)\n",
    "        meters_per_deg_lat = 111320\n",
    "        meters_per_deg_lon = 111320 * np.cos(lat_rad)\n",
    "        \n",
    "        half_size_lat = half_size_meters / meters_per_deg_lat\n",
    "        half_size_lon = half_size_meters / meters_per_deg_lon\n",
    "        \n",
    "        west = longitude - half_size_lon\n",
    "        east = longitude + half_size_lon\n",
    "        south = latitude - half_size_lat\n",
    "        north = latitude + half_size_lat\n",
    "        \n",
    "        region = ee.Geometry.Rectangle([west, south, east, north])\n",
    "        \n",
    "        # Check AlphaEarth collection\n",
    "        embedding_collection = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n",
    "        \n",
    "        # Check total collection size\n",
    "        total_size = embedding_collection.size().getInfo()\n",
    "        logger.info(f'Total AlphaEarth images: {total_size}')\n",
    "        \n",
    "        # Check filtered collection\n",
    "        filtered_collection = embedding_collection.filterBounds(region).filterDate(\n",
    "            f'{year}-01-01', f'{year+1}-01-01'\n",
    "        )\n",
    "        \n",
    "        filtered_size = filtered_collection.size().getInfo()\n",
    "        logger.info(f'Filtered images for {year}: {filtered_size}')\n",
    "        \n",
    "        if filtered_size > 0:\n",
    "            # Get image info\n",
    "            image = filtered_collection.first()\n",
    "            image_info = image.getInfo()\n",
    "            logger.info(f'Image properties: {list(image_info.keys())}')\n",
    "            \n",
    "            # Check band names\n",
    "            if 'bands' in image_info:\n",
    "                band_names = [band['id'] for band in image_info['bands']]\n",
    "                logger.info(f'Available bands: {band_names[:10]}... (showing first 10)')\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f'No AlphaEarth data available for {year}')\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f'Diagnosis failed: {e}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 08:43:43,771 - INFO -  Diagnosing AlphaEarth data for (42.9575, -91.6240) in 2017\n",
      "2025-10-22 08:43:43,772 - ERROR - Diagnosis failed: Earth Engine client library not initialized. See http://goo.gle/ee-auth.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing AlphaEarth data availability for first feature...\n",
      "First feature: lat=42.95753, lon=-91.62403, peak_date=2016-08-25 12:00:00, download_year=2017\n"
     ]
    }
   ],
   "source": [
    "print(' Testing AlphaEarth data availability for first feature...')\n",
    "\n",
    "# set relative path\n",
    "base_dir = os.getcwd()\n",
    "test_file = os.path.join(base_dir, 'csv_data', 'Unified_Peak_Data_2016_2017_with_ID(1006).csv')\n",
    "\n",
    "# read CSV file\n",
    "df = pd.read_csv(test_file)\n",
    "\n",
    "# acquiring data of the first row\n",
    "first_row = df.iloc[0]\n",
    "lat = first_row['latitude']\n",
    "lon = first_row['longitude']\n",
    "peak_date = first_row['peak_date']\n",
    "\n",
    "# calculate which year to download\n",
    "download_year = get_download_year(peak_date, 'later')\n",
    "print(f'First feature: lat={lat}, lon={lon}, peak_date={peak_date}, download_year={download_year}')\n",
    "\n",
    "# Diagnose AlphaEarth data availability\n",
    "if download_year:\n",
    "    diagnose_embedding_availability(lat, lon, download_year)\n",
    "else:\n",
    "    print(' Could not determine download year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 08:43:44,347 - INFO - Google Earth Engine initialized successfully with service account\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GEE initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize GEE\n",
    "if not initialize_gee():\n",
    "    raise Exception(\"Failed to initialize Google Earth Engine\")\n",
    "\n",
    "print(' GEE initialized successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early\n",
      "Created directory: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "\n",
    "# setting output path\n",
    "output_dirs = [\n",
    "    os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_early'),\n",
    "    os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_later')\n",
    "]\n",
    "\n",
    "# creat folder\n",
    "for output_dir in output_dirs:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f'Created directory: {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File configurations defined:\n",
      "   1. 2016–2017 Peak Data (later strategy)\n",
      "      File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/Unified_Peak_Data_2016_2017_with_ID(1006).csv\n",
      "      Strategy: later\n",
      "      Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later_example\n",
      "      Prefix: gage_\n",
      "\n",
      "   2. 2018+ Peak Data (early strategy)\n",
      "      File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\n",
      "      Strategy: early\n",
      "      Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early_example\n",
      "      Prefix: gage_\n",
      "\n",
      "   3. 2016–2017 Matched Records (later strategy)\n",
      "      File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/matched_records_1947_with_ID_2016_2017(1006).csv\n",
      "      Strategy: later\n",
      "      Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later_example\n",
      "      Prefix: HWM_\n",
      "\n",
      "   4. 2018+ Matched Records (early strategy)\n",
      "      File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/matched_records_698_with_ID_2018_and_later(1006).csv\n",
      "      Strategy: early\n",
      "      Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early_example\n",
      "      Prefix: HWM_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a file configuration list (for embedded downloads)\n",
    "file_configs = [\n",
    "    {\n",
    "        'description': '2016–2017 Peak Data (later strategy)',\n",
    "        'file': os.path.join(base_dir, 'csv_data', 'Unified_Peak_Data_2016_2017_with_ID(1006).csv'),\n",
    "        'strategy': 'later',\n",
    "        'output_dir': os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_later_example'),\n",
    "        'prefix': 'gage_'\n",
    "    },\n",
    "    {\n",
    "        'description': '2018+ Peak Data (early strategy)',\n",
    "        'file': os.path.join(base_dir, 'csv_data', 'Unified_Peak_Data_2018_and_later_with_ID(1006).csv'),\n",
    "        'strategy': 'early',\n",
    "        'output_dir': os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_early_example'),\n",
    "        'prefix': 'gage_'\n",
    "    },\n",
    "    {\n",
    "        'description': '2016–2017 Matched Records (later strategy)',\n",
    "        'file': os.path.join(base_dir, 'csv_data', 'matched_records_1947_with_ID_2016_2017(1006).csv'),\n",
    "        'strategy': 'later',\n",
    "        'output_dir': os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_later_example'),\n",
    "        'prefix': 'HWM_'\n",
    "    },\n",
    "    {\n",
    "        'description': '2018+ Matched Records (early strategy)',\n",
    "        'file': os.path.join(base_dir, 'csv_data', 'matched_records_698_with_ID_2018_and_later(1006).csv'),\n",
    "        'strategy': 'early',\n",
    "        'output_dir': os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_early_example'),\n",
    "        'prefix': 'HWM_'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Print Configuration Overview\n",
    "print('File configurations defined:')\n",
    "for i, config in enumerate(file_configs, 1):\n",
    "    print(f'   {i}. {config[\"description\"]}')\n",
    "    print(f'      File: {config[\"file\"]}')\n",
    "    print(f'      Strategy: {config[\"strategy\"]}')\n",
    "    print(f'      Output: {config[\"output_dir\"]}')\n",
    "    print(f'      Prefix: {config[\"prefix\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 08:43:44,414 - INFO -    Processing: 2016–2017 Peak Data (later strategy)\n",
      "2025-10-22 08:43:44,414 - INFO -    File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/Unified_Peak_Data_2016_2017_with_ID(1006).csv\n",
      "2025-10-22 08:43:44,415 - INFO -    Strategy: later\n",
      "2025-10-22 08:43:44,415 - INFO -    Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later_example\n",
      "2025-10-22 08:43:44,416 - INFO -    Prefix: gage_\n",
      "2025-10-22 08:43:44,417 - INFO -    Max features: 3\n",
      "2025-10-22 08:43:44,421 - INFO - 📊 Records: 3\n",
      "2025-10-22 08:43:44,424 - INFO - 📋 Valid features for download: 0\n",
      "2025-10-22 08:43:44,424 - INFO -  No valid features to download\n",
      "2025-10-22 08:43:44,425 - INFO -    Processing: 2018+ Peak Data (early strategy)\n",
      "2025-10-22 08:43:44,425 - INFO -    File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\n",
      "2025-10-22 08:43:44,426 - INFO -    Strategy: early\n",
      "2025-10-22 08:43:44,426 - INFO -    Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early_example\n",
      "2025-10-22 08:43:44,426 - INFO -    Prefix: gage_\n",
      "2025-10-22 08:43:44,427 - INFO -    Max features: 3\n",
      "2025-10-22 08:43:44,431 - INFO - 📊 Records: 3\n",
      "2025-10-22 08:43:44,433 - INFO - 📋 Valid features for download: 0\n",
      "2025-10-22 08:43:44,434 - INFO -  No valid features to download\n",
      "2025-10-22 08:43:44,434 - INFO -    Processing: 2016–2017 Matched Records (later strategy)\n",
      "2025-10-22 08:43:44,435 - INFO -    File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/matched_records_1947_with_ID_2016_2017(1006).csv\n",
      "2025-10-22 08:43:44,435 - INFO -    Strategy: later\n",
      "2025-10-22 08:43:44,436 - INFO -    Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later_example\n",
      "2025-10-22 08:43:44,436 - INFO -    Prefix: HWM_\n",
      "2025-10-22 08:43:44,436 - INFO -    Max features: 3\n",
      "2025-10-22 08:43:44,442 - INFO - 📊 Records: 3\n",
      "2025-10-22 08:43:44,444 - INFO - 📋 Valid features for download: 0\n",
      "2025-10-22 08:43:44,445 - INFO -  No valid features to download\n",
      "2025-10-22 08:43:44,445 - INFO -    Processing: 2018+ Matched Records (early strategy)\n",
      "2025-10-22 08:43:44,446 - INFO -    File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/matched_records_698_with_ID_2018_and_later(1006).csv\n",
      "2025-10-22 08:43:44,446 - INFO -    Strategy: early\n",
      "2025-10-22 08:43:44,446 - INFO -    Output: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early_example\n",
      "2025-10-22 08:43:44,447 - INFO -    Prefix: HWM_\n",
      "2025-10-22 08:43:44,447 - INFO -    Max features: 3\n",
      "2025-10-22 08:43:44,453 - INFO - 📊 Records: 3\n",
      "2025-10-22 08:43:44,455 - INFO - 📋 Valid features for download: 0\n",
      "2025-10-22 08:43:44,455 - INFO -  No valid features to download\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-File AlphaEarth Embedding Download (Limited to 3 files, 3 features each)\n",
      "======================================================================\n",
      "\n",
      " Processing file 1/4: 2016–2017 Peak Data (later strategy)\n",
      " Results: 0 successful, 0 failed\n",
      "\n",
      " Processing file 2/4: 2018+ Peak Data (early strategy)\n",
      " Results: 0 successful, 0 failed\n",
      "\n",
      " Processing file 3/4: 2016–2017 Matched Records (later strategy)\n",
      " Results: 0 successful, 0 failed\n",
      "\n",
      " Processing file 4/4: 2018+ Matched Records (early strategy)\n",
      " Results: 0 successful, 0 failed\n",
      "\n",
      " All embedding downloads completed!\n",
      " Final Summary:\n",
      "   Total successful: 0\n",
      "   Total failed: 0\n",
      "   Total skipped (existing): 0\n",
      "   Overall success rate: N/A (no downloads attempted)\n"
     ]
    }
   ],
   "source": [
    "# Start downloading embeddings\n",
    "#print('Starting Multi-File AlphaEarth Embedding Download')\n",
    "#print('=' * 50)\n",
    "# Start downloading embeddings (only first 3 files, 3 features each)\n",
    "print('Starting Multi-File AlphaEarth Embedding Download (Limited to 3 files, 3 features each)')\n",
    "print('=' * 70)\n",
    "\n",
    "total_stats = {'successful': 0, 'failed': 0, 'skipped': 0}\n",
    "\n",
    "for i, file_config in enumerate(file_configs, 1):\n",
    "    print(f'\\n Processing file {i}/4: {file_config[\"description\"]}')\n",
    "    \n",
    "    # Process file\n",
    "    #stats = process_single_file(file_config)\n",
    "    # Process file (max 3 features per file)\n",
    "    stats = process_single_file(file_config, max_features=3)    \n",
    "    # Update totals\n",
    "    for key in total_stats:\n",
    "        total_stats[key] += stats[key]\n",
    "    \n",
    "    print(f' Results: {stats[\"successful\"]} successful, {stats[\"failed\"]} failed')\n",
    "\n",
    "print(f'\\n All embedding downloads completed!')\n",
    "print(f' Final Summary:')\n",
    "print(f'   Total successful: {total_stats[\"successful\"]:,}')\n",
    "print(f'   Total failed: {total_stats[\"failed\"]:,}')\n",
    "print(f'   Total skipped (existing): {total_stats[\"skipped\"]:,}')\n",
    "\n",
    "# Calculate success rate with zero division protection\n",
    "total_attempted = total_stats[\"successful\"] + total_stats[\"failed\"]\n",
    "if total_attempted > 0:\n",
    "    success_rate = total_stats[\"successful\"] / total_attempted * 100\n",
    "    print(f'   Overall success rate: {success_rate:.1f}%')\n",
    "else:\n",
    "    print(f'   Overall success rate: N/A (no downloads attempted)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Checking downloaded files:\n",
      "   /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early: 0 files\n",
      "   /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later: 0 files\n"
     ]
    }
   ],
   "source": [
    "# Check downloaded files\n",
    "print('\\n  Checking downloaded files:')\n",
    "base_dir = os.getcwd()\n",
    "output_dirs = [\n",
    "    os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_early_example'),\n",
    "    os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_later_example')\n",
    "]\n",
    "\n",
    "for output_dir in output_dirs:\n",
    "    if os.path.exists(output_dir):\n",
    "        files = [f for f in os.listdir(output_dir) if f.endswith('.npz')]\n",
    "        print(f'   {output_dir}: {len(files):,} files')\n",
    "    else:\n",
    "        print(f'   {output_dir}: Directory not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Verifying file format (sample check):\n",
      "\n",
      " File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early_example/gage_12003.npz\n",
      "   Keys: ['image_data', 'feature_id', 'centroid_lon', 'centroid_lat', 'year', 'num_images', 'band_names', 'flipud_applied']\n",
      "   Image shape: (64, 256, 256)\n",
      "   Data type: float64\n",
      "   Feature ID: 12003\n",
      "   Year: 2017\n",
      "   Band names: ['A00' 'A01' 'A02' 'A03' 'A04']... (first 5)\n",
      "\n",
      " File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later_example/gage_10002.npz\n",
      "   Keys: ['image_data', 'feature_id', 'centroid_lon', 'centroid_lat', 'year', 'num_images', 'band_names', 'flipud_applied']\n",
      "   Image shape: (64, 254, 255)\n",
      "   Data type: float64\n",
      "   Feature ID: 10002\n",
      "   Year: 2017\n",
      "   Band names: ['A00' 'A01' 'A02' 'A03' 'A04']... (first 5)\n"
     ]
    }
   ],
   "source": [
    "# Verify file format (sample check)\n",
    "print('\\n Verifying file format (sample check):')\n",
    "base_dir = os.getcwd()\n",
    "output_dirs = [\n",
    "    os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_early_example'),\n",
    "    os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_later_example')\n",
    "]\n",
    "sample_files = []\n",
    "for output_dir in output_dirs:\n",
    "    if os.path.exists(output_dir):\n",
    "        files = [f for f in os.listdir(output_dir) if f.endswith('.npz')]\n",
    "        if files:\n",
    "            sample_files.append(os.path.join(output_dir, files[0]))\n",
    "\n",
    "for file_path in sample_files[:2]:\n",
    "    print(f'\\n File: {file_path}')\n",
    "    try:\n",
    "        data = np.load(file_path)\n",
    "        print(f'   Keys: {list(data.keys())}')\n",
    "        if 'image_data' in data:\n",
    "            print(f'   Image shape: {data[\"image_data\"].shape}')\n",
    "            print(f'   Data type: {data[\"image_data\"].dtype}')\n",
    "        if 'feature_id' in data:\n",
    "            print(f'   Feature ID: {data[\"feature_id\"]}')\n",
    "        if 'year' in data:\n",
    "            print(f'   Year: {data[\"year\"]}')\n",
    "        if 'band_names' in data:\n",
    "            print(f'   Band names: {data[\"band_names\"][:5]}... (first 5)')\n",
    "    except Exception as e:\n",
    "        print(f'   Error: {e}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
