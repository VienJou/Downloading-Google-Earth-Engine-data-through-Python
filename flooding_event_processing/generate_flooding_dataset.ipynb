{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flooding Event Dataset Generation \n",
        "\n",
        "This notebook generates a comprehensive JSON dataset from flooding event data sources with optimized code structure.\n",
        "\n",
        "## Data Sources:\n",
        "1. **Unified_Peak_Data_2016_2017_with_ID(1006).csv** - Gage data for 2016-2017\n",
        "2. **Unified_Peak_Data_2018_and_later_with_ID(1006).csv** - Gage data for 2018+\n",
        "3. **matched_records_1947_with_ID_2016_2017(1006).csv** - HWM data for 2016-2017\n",
        "4. **matched_records_698_with_ID_2018_and_later(1006).csv** - HWM data for 2018+\n",
        "\n",
        "## File Mapping:\n",
        "- **Numpy files (embedding)**: \n",
        "  - embedding_1Y_later/ - for 2016-2017 data\n",
        "  - embedding_1Y_early/ - for 2018+ data\n",
        "- **Weather files**: \n",
        "  - gage_2016_2017_24h/ and hwm_2016_2017_24h/ - for 2016-2017 data\n",
        "  - gage_2018_later_24h/ and hwm_2018_later_24h/ - for 2018+ data\n",
        "\n",
        "## Naming Convention:\n",
        "- **Gage data**: gage_[ID].npz, gage_[ID].csv\n",
        "- **HWM data**: HWM_[ID].npz, HWM_[ID].csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and File Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File paths configured:\n",
            "\n",
            "Input CSV files:\n",
            "  gage_2016_2017: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv\n",
            "  gage_2018_later: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\n",
            "  hwm_2016_2017: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv\n",
            "  hwm_2018_later: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv\n",
            "\n",
            "Numpy directories:\n",
            "  embedding_1Y_later: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later\n",
            "  embedding_1Y_early: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early\n",
            "\n",
            "Weather directories:\n",
            "  gage_2016_2017: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/gage_2016_2017_24h\n",
            "  gage_2018_later: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/gage_2018_later_24h\n",
            "  hwm_2016_2017: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/hwm_2016_2017_24h\n",
            "  hwm_2018_later: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/hwm_2018_later_24h\n",
            "\n",
            "Output directory: /u/wz53/alphaearth/Flooding_event_/Flood_dataset_example\n"
          ]
        }
      ],
      "source": [
        "# Define input CSV files\n",
        "csv_files = {\n",
        "    'gage_2016_2017': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv\",\n",
        "    'gage_2018_later': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\",\n",
        "    'hwm_2016_2017': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv\",\n",
        "    'hwm_2018_later': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv\"\n",
        "}\n",
        "\n",
        "# Define directories for numpy files (embeddings)\n",
        "numpy_dirs = {\n",
        "    'embedding_1Y_later': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later\",\n",
        "    'embedding_1Y_early': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early\"\n",
        "}\n",
        "\n",
        "# Define directories for weather files\n",
        "weather_dirs = {\n",
        "    'gage_2016_2017': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/gage_2016_2017_24h\",\n",
        "    'gage_2018_later': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/gage_2018_later_24h\",\n",
        "    'hwm_2016_2017': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/hwm_2016_2017_24h\",\n",
        "    'hwm_2018_later': \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset/hwm_2018_later_24h\"\n",
        "}\n",
        "\n",
        "# Output directory\n",
        "output_dir = \"/u/wz53/alphaearth/Flooding_event_/Flood_dataset_example\"\n",
        "\n",
        "print(\"File paths configured:\")\n",
        "print(\"\\nInput CSV files:\")\n",
        "for key, path in csv_files.items():\n",
        "    print(f\"  {key}: {path}\")\n",
        "\n",
        "print(\"\\nNumpy directories:\")\n",
        "for key, path in numpy_dirs.items():\n",
        "    print(f\"  {key}: {path}\")\n",
        "\n",
        "print(\"\\nWeather directories:\")\n",
        "for key, path in weather_dirs.items():\n",
        "    print(f\"  {key}: {path}\")\n",
        "\n",
        "print(f\"\\nOutput directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utility functions defined successfully!\n",
            "CORRECTED VERSION: Now properly extracts elevation_m and height_above_gnd_m from CSV files\n"
          ]
        }
      ],
      "source": [
        "def scan_directory_files(directory):\n",
        "    \"\"\"\n",
        "    Scan a directory and return a dictionary mapping file IDs to full paths.\n",
        "    \n",
        "    Args:\n",
        "        directory (str): Directory path to scan\n",
        "        \n",
        "    Returns:\n",
        "        dict: {file_id: full_path} mapping\n",
        "    \"\"\"\n",
        "    file_map = {}\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Warning: Directory not found: {directory}\")\n",
        "        return file_map\n",
        "    \n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.npz') or filename.endswith('.csv'):\n",
        "            # Extract ID from filename\n",
        "            # Format: gage_[ID].ext or HWM_[ID].ext\n",
        "            basename = filename.rsplit('.', 1)[0]  # Remove extension\n",
        "            if '_' in basename:\n",
        "                file_id = basename.split('_', 1)[1]  # Get part after first underscore\n",
        "                full_path = os.path.join(directory, filename)\n",
        "                file_map[file_id] = full_path\n",
        "    \n",
        "    return file_map\n",
        "\n",
        "def find_matching_files(record_id, data_type, period):\n",
        "    \"\"\"\n",
        "    Find matching numpy and weather files based on ID, data type, and period.\n",
        "    \n",
        "    Args:\n",
        "        record_id (str): Record ID to match\n",
        "        data_type (str): 'gage' or 'hwm'\n",
        "        period (str): '2016_2017' or '2018_later'\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (numpy_file_path, weather_file_path) with full paths or (None, None)\n",
        "    \"\"\"\n",
        "    numpy_file = None\n",
        "    weather_file = None\n",
        "    \n",
        "    # Determine numpy directory based on period\n",
        "    if period == '2016_2017':\n",
        "        numpy_dir = numpy_dirs['embedding_1Y_later']\n",
        "    else:  # 2018_later\n",
        "        numpy_dir = numpy_dirs['embedding_1Y_early']\n",
        "    \n",
        "    # Determine weather directory based on data type and period\n",
        "    weather_key = f\"{data_type}_{period}\"\n",
        "    weather_dir = weather_dirs.get(weather_key)\n",
        "    \n",
        "    if not weather_dir:\n",
        "        return None, None\n",
        "    \n",
        "    # Build expected filenames\n",
        "    if data_type == 'gage':\n",
        "        numpy_filename = f\"gage_{record_id}.npz\"\n",
        "        weather_filename = f\"gage_{record_id}.csv\"\n",
        "    else:  # hwm\n",
        "        numpy_filename = f\"HWM_{record_id}.npz\"\n",
        "        weather_filename = f\"HWM_{record_id}.csv\"\n",
        "    \n",
        "    # Check if files exist\n",
        "    numpy_path = os.path.join(numpy_dir, numpy_filename)\n",
        "    weather_path = os.path.join(weather_dir, weather_filename)\n",
        "    \n",
        "    if os.path.exists(numpy_path):\n",
        "        numpy_file = numpy_path\n",
        "    \n",
        "    if os.path.exists(weather_path):\n",
        "        weather_file = weather_path\n",
        "    \n",
        "    return numpy_file, weather_file\n",
        "\n",
        "def safe_float(value):\n",
        "    \"\"\"\n",
        "    Safely convert value to float, return None if invalid.\n",
        "    \n",
        "    Args:\n",
        "        value: Value to convert (can be number, string, or None)\n",
        "        \n",
        "    Returns:\n",
        "        float: Converted value, or None if invalid\n",
        "    \"\"\"\n",
        "    if value is None or pd.isna(value) or str(value).strip() in ['', 'nan', 'NaN', 'None']:\n",
        "        return None\n",
        "    try:\n",
        "        return float(value)\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "def extract_record(row, data_type):\n",
        "    \"\"\"\n",
        "    Extract required fields from CSV row (CORRECTED VERSION).\n",
        "    \n",
        "    IMPORTANT CHANGES:\n",
        "    - For gage data: Uses elevation_m and height_above_gnd_m from CSV (already in meters)\n",
        "    - For hwm data: Uses elev_ft_m as elevation_m and height_above_gnd_m from CSV (already converted)\n",
        "    \n",
        "    Args:\n",
        "        row (dict): CSV row data\n",
        "        data_type (str): 'gage' or 'hwm'\n",
        "        \n",
        "    Returns:\n",
        "        dict: Processed record\n",
        "    \"\"\"\n",
        "    if data_type == 'gage':\n",
        "        # Gage data (Unified_Peak_Data)\n",
        "        # CSV files have been updated with height_above_gnd_m column (converted from ft to m)\n",
        "        return {\n",
        "            \"ID\": row.get(\"ID\", \"\"),\n",
        "            \"latitude\": safe_float(row.get(\"latitude\")),\n",
        "            \"longitude\": safe_float(row.get(\"longitude\")),\n",
        "            \"Ground_Elevation_m\": safe_float(row.get(\"Ground_Elevation_m\")),\n",
        "            \"site_no\": row.get(\"site_no\", \"\"),\n",
        "            \"station_id\": row.get(\"station_id\", \"\"),\n",
        "            \"peak_date\": row.get(\"peak_date\", \"\"),\n",
        "            \"elevation_m\": safe_float(row.get(\"elevation_m\")),\n",
        "            \"peak_stage\": safe_float(row.get(\"peak_stage\")),\n",
        "            \"event\": row.get(\"event\", \"\"),\n",
        "            \"source\": row.get(\"source\", \"\"),\n",
        "            \"height_above_gnd_m\": safe_float(row.get(\"height_above_gnd_m\")),  # Use converted column\n",
        "            \"data_type\": \"gage\"\n",
        "        }\n",
        "    else:\n",
        "        # HWM data (matched_records)\n",
        "        # CORRECTED: Use elev_ft_m as elevation_m, and height_above_gnd_m (both already converted to meters)\n",
        "        return {\n",
        "            \"ID\": row.get(\"ID\", \"\"),\n",
        "            \"latitude\": safe_float(row.get(\"latitude\")),\n",
        "            \"longitude\": safe_float(row.get(\"longitude\")),\n",
        "            \"site_no\": row.get(\"site_no\", \"\"),\n",
        "            \"elevation_m\": safe_float(row.get(\"elev_ft_m\")),  # CORRECTED: Use elev_ft_m\n",
        "            \"peak_stage\": safe_float(row.get(\"elev_ft_m\")),   # HWM uses same value for peak_stage\n",
        "            \"height_above_gnd_m\": safe_float(row.get(\"height_above_gnd_m\")),  # CORRECTED: Use converted column\n",
        "            \"data_type\": \"hwm\"\n",
        "        }\n",
        "\n",
        "print(\"Utility functions defined successfully!\")\n",
        "print(\"CORRECTED VERSION: Now properly extracts elevation_m and height_above_gnd_m from CSV files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Available Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning directories and counting files...\n",
            "============================================================\n",
            "Numpy directory: embedding_1Y_later\n",
            "  Path: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_later\n",
            "  Files: 3896\n",
            "  Sample: HWM_1.npz, HWM_10.npz, HWM_100.npz\n",
            "Numpy directory: embedding_1Y_early\n",
            "  Path: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/embedding_1Y_early\n",
            "  Files: 2995\n",
            "  Sample: HWM_1949.npz, HWM_1950.npz, HWM_1951.npz\n",
            "\n",
            "Weather directory: gage_2016_2017\n",
            "  Path: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/gage_2016_2017_24h\n",
            "  Files: 1949\n",
            "  Sample: gage_10001.csv, gage_10002.csv, gage_10003.csv\n",
            "Weather directory: gage_2018_later\n",
            "  Path: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/gage_2018_later_24h\n",
            "  Files: 2297\n",
            "  Sample: gage_12002.csv, gage_12003.csv, gage_12004.csv\n",
            "Weather directory: hwm_2016_2017\n",
            "  Path: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/hwm_2016_2017_24h\n",
            "  Files: 1947\n",
            "  Sample: HWM_1.csv, HWM_10.csv, HWM_100.csv\n",
            "Weather directory: hwm_2018_later\n",
            "  Path: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/hwm_2018_later_24h\n",
            "  Files: 698\n",
            "  Sample: HWM_1949.csv, HWM_1950.csv, HWM_1951.csv\n",
            "\n",
            "Directory scanning completed!\n"
          ]
        }
      ],
      "source": [
        "# Count files in each directory\n",
        "print(\"Scanning directories and counting files...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Scan numpy directories\n",
        "numpy_file_counts = {}\n",
        "for key, directory in numpy_dirs.items():\n",
        "    if os.path.exists(directory):\n",
        "        files = [f for f in os.listdir(directory) if f.endswith('.npz')]\n",
        "        numpy_file_counts[key] = len(files)\n",
        "        print(f\"Numpy directory: {key}\")\n",
        "        print(f\"  Path: {directory}\")\n",
        "        print(f\"  Files: {len(files)}\")\n",
        "        if files[:3]:\n",
        "            print(f\"  Sample: {', '.join(files[:3])}\")\n",
        "    else:\n",
        "        numpy_file_counts[key] = 0\n",
        "        print(f\"Numpy directory: {key} - NOT FOUND\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Scan weather directories\n",
        "weather_file_counts = {}\n",
        "for key, directory in weather_dirs.items():\n",
        "    if os.path.exists(directory):\n",
        "        files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
        "        weather_file_counts[key] = len(files)\n",
        "        print(f\"Weather directory: {key}\")\n",
        "        print(f\"  Path: {directory}\")\n",
        "        print(f\"  Files: {len(files)}\")\n",
        "        if files[:3]:\n",
        "            print(f\"  Sample: {', '.join(files[:3])}\")\n",
        "    else:\n",
        "        weather_file_counts[key] = 0\n",
        "        print(f\"Weather directory: {key} - NOT FOUND\")\n",
        "\n",
        "print()\n",
        "print(\"Directory scanning completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process All CSV Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing all CSV files...\n",
            "============================================================\n",
            "\n",
            "Processing: gage_2016_2017\n",
            "  Records: 1949\n",
            "  Processed: 1949 records\n",
            "\n",
            "Processing: gage_2018_later\n",
            "  Records: 2297\n",
            "  Processed: 2297 records\n",
            "\n",
            "Processing: hwm_2016_2017\n",
            "  Records: 1947\n",
            "  Processed: 1947 records\n",
            "\n",
            "Processing: hwm_2018_later\n",
            "  Records: 698\n",
            "  Processed: 698 records\n",
            "\n",
            "============================================================\n",
            "Processing completed: 6891 total records\n",
            "  Gage: 4246, HWM: 2645\n",
            "  With numpy: 6891 (100.0%)\n",
            "  With weather: 6891 (100.0%)\n",
            "  With both files: 6891 (100.0%)\n",
            "  With both elevations: 3720 (54.0%)\n"
          ]
        }
      ],
      "source": [
        "def calculate_comprehensive_stats(all_data):\n",
        "    \"\"\"Calculate all statistics in one pass to avoid redundancy\"\"\"\n",
        "    stats = {\n",
        "        'total_records': len(all_data),\n",
        "        'gage_records': 0,\n",
        "        'hwm_records': 0,\n",
        "        'records_with_numpy': 0,\n",
        "        'records_with_weather': 0,\n",
        "        'records_with_both': 0,\n",
        "        'records_with_elevation': 0,\n",
        "        'records_with_height': 0,\n",
        "        'records_with_both_elev_height': 0,\n",
        "        'by_period': {'2016_2017': 0, '2018_later': 0},\n",
        "        'by_type_period': {}\n",
        "    }\n",
        "    \n",
        "    for record in all_data:\n",
        "        # Basic counts\n",
        "        if record['data_type'] == 'gage':\n",
        "            stats['gage_records'] += 1\n",
        "        else:\n",
        "            stats['hwm_records'] += 1\n",
        "        \n",
        "        # File availability\n",
        "        if record['numpy_file']:\n",
        "            stats['records_with_numpy'] += 1\n",
        "        if record['weather_file']:\n",
        "            stats['records_with_weather'] += 1\n",
        "        if record['numpy_file'] and record['weather_file']:\n",
        "            stats['records_with_both'] += 1\n",
        "        \n",
        "        # Elevation data\n",
        "        if record.get('elevation_m') is not None:\n",
        "            stats['records_with_elevation'] += 1\n",
        "        if record.get('height_above_gnd_m') is not None:\n",
        "            stats['records_with_height'] += 1\n",
        "        if record.get('elevation_m') is not None and record.get('height_above_gnd_m') is not None:\n",
        "            stats['records_with_both_elev_height'] += 1\n",
        "        \n",
        "        # Period counts\n",
        "        period = record.get('period', 'unknown')\n",
        "        if period in stats['by_period']:\n",
        "            stats['by_period'][period] += 1\n",
        "        \n",
        "        # Type-period combinations\n",
        "        key = f\"{record['data_type']}_{period}\"\n",
        "        stats['by_type_period'][key] = stats['by_type_period'].get(key, 0) + 1\n",
        "    \n",
        "    return stats\n",
        "\n",
        "print(\"Processing all CSV files...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_data = []\n",
        "\n",
        "# Process each CSV file\n",
        "for file_key, csv_path in csv_files.items():\n",
        "    print(f\"\\nProcessing: {file_key}\")\n",
        "    \n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"  ERROR: File not found!\")\n",
        "        continue\n",
        "    \n",
        "    # Determine data type and period\n",
        "    data_type = 'gage' if 'gage' in file_key else 'hwm'\n",
        "    period = '2016_2017' if '2016_2017' in file_key else '2018_later'\n",
        "    \n",
        "    # Read and process CSV file\n",
        "    df = pd.read_csv(csv_path, low_memory=False)\n",
        "    print(f\"  Records: {len(df)}\")\n",
        "    \n",
        "    # Process each record\n",
        "    for idx, row in df.iterrows():\n",
        "        record = extract_record(row.to_dict(), data_type)\n",
        "        record_id = str(row.get('ID', ''))\n",
        "        \n",
        "        if not record_id:\n",
        "            continue\n",
        "        \n",
        "        # Find matching files and add metadata\n",
        "        numpy_file, weather_file = find_matching_files(record_id, data_type, period)\n",
        "        record.update({\n",
        "            'numpy_file': numpy_file,\n",
        "            'weather_file': weather_file,\n",
        "            'period': period\n",
        "        })\n",
        "        \n",
        "        all_data.append(record)\n",
        "    \n",
        "    print(f\"  Processed: {len(df)} records\")\n",
        "\n",
        "# Calculate all statistics once\n",
        "stats = calculate_comprehensive_stats(all_data)\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"Processing completed: {stats['total_records']} total records\")\n",
        "print(f\"  Gage: {stats['gage_records']}, HWM: {stats['hwm_records']}\")\n",
        "print(f\"  With numpy: {stats['records_with_numpy']} ({stats['records_with_numpy']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"  With weather: {stats['records_with_weather']} ({stats['records_with_weather']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"  With both files: {stats['records_with_both']} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"  With both elevations: {stats['records_with_both_elev_height']} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Statistics and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Statistics Summary\n",
            "============================================================\n",
            "\n",
            "Overall Statistics:\n",
            "  Total records: 6891\n",
            "  Gage records: 4246\n",
            "  HWM records: 2645\n",
            "\n",
            "Detailed Statistics by Type-Period:\n",
            "  gage_2016_2017: 1949 records\n",
            "  gage_2018_later: 2297 records\n",
            "  hwm_2016_2017: 1947 records\n",
            "  hwm_2018_later: 698 records\n",
            "\n",
            "File Matching Summary:\n",
            "  Records with numpy files: 6891 (100.0%)\n",
            "  Records with weather files: 6891 (100.0%)\n",
            "  Records with both files: 6891 (100.0%)\n",
            "\n",
            "Elevation Data Summary:\n",
            "  Records with elevation_m: 6855 (99.5%)\n",
            "  Records with height_above_gnd_m: 3743 (54.3%)\n",
            "  Records with both elevations: 3720 (54.0%)\n",
            "\n",
            "Data Completeness:\n",
            "  Records ready for analysis: 6891/6891\n"
          ]
        }
      ],
      "source": [
        "print(\"Data Statistics Summary\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nOverall Statistics:\")\n",
        "print(f\"  Total records: {stats['total_records']}\")\n",
        "print(f\"  Gage records: {stats['gage_records']}\")\n",
        "print(f\"  HWM records: {stats['hwm_records']}\")\n",
        "\n",
        "print(f\"\\nDetailed Statistics by Type-Period:\")\n",
        "for key, count in stats['by_type_period'].items():\n",
        "    print(f\"  {key}: {count} records\")\n",
        "\n",
        "print(f\"\\nFile Matching Summary:\")\n",
        "print(f\"  Records with numpy files: {stats['records_with_numpy']} ({stats['records_with_numpy']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"  Records with weather files: {stats['records_with_weather']} ({stats['records_with_weather']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"  Records with both files: {stats['records_with_both']} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nElevation Data Summary:\")\n",
        "print(f\"  Records with elevation_m: {stats['records_with_elevation']} ({stats['records_with_elevation']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"  Records with height_above_gnd_m: {stats['records_with_height']} ({stats['records_with_height']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"  Records with both elevations: {stats['records_with_both_elev_height']} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nData Completeness:\")\n",
        "print(f\"  Records ready for analysis: {stats['records_with_both']}/{stats['total_records']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate JSON Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating JSON dataset...\n",
            "Output file: /u/wz53/alphaearth/Flooding_event_/Flood_dataset_example/flooding_dataset_optimized_20251020_233912.json\n",
            "✓ JSON dataset generated successfully!\n",
            "  File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset_example/flooding_dataset_optimized_20251020_233912.json\n",
            "  Size: 3.98 MB\n",
            "  Records: 6,891\n",
            "  Records with both files: 6,891 (100.0%)\n",
            "  Records with both elevations: 3,720 (54.0%)\n"
          ]
        }
      ],
      "source": [
        "def create_dataset_metadata(stats, numpy_file_counts, weather_file_counts):\n",
        "    \"\"\"Create comprehensive metadata using pre-calculated statistics\"\"\"\n",
        "    return {\n",
        "        \"description\": \"Comprehensive flooding event dataset with file paths (Optimized Version)\",\n",
        "        \"version\": \"5.0\",\n",
        "        \"generated_timestamp\": datetime.now().isoformat(),\n",
        "        \"total_records\": stats['total_records'],\n",
        "        \"gage_records\": stats['gage_records'],\n",
        "        \"hwm_records\": stats['hwm_records'],\n",
        "        \"data_sources\": csv_files,\n",
        "        \"file_directories\": {\n",
        "            \"numpy\": numpy_dirs,\n",
        "            \"weather\": weather_dirs\n",
        "        },\n",
        "        \"file_counts\": {\n",
        "            \"numpy_files\": numpy_file_counts,\n",
        "            \"weather_files\": weather_file_counts\n",
        "        },\n",
        "        \"statistics\": {\n",
        "            \"records_with_numpy\": stats['records_with_numpy'],\n",
        "            \"records_with_weather\": stats['records_with_weather'],\n",
        "            \"records_with_both\": stats['records_with_both'],\n",
        "            \"records_with_elevation_m\": stats['records_with_elevation'],\n",
        "            \"records_with_height_above_gnd_m\": stats['records_with_height'],\n",
        "            \"records_with_both_elev_height\": stats['records_with_both_elev_height'],\n",
        "            \"matching_rates\": {\n",
        "                \"numpy\": f\"{stats['records_with_numpy']/stats['total_records']*100:.2f}%\",\n",
        "                \"weather\": f\"{stats['records_with_weather']/stats['total_records']*100:.2f}%\",\n",
        "                \"both\": f\"{stats['records_with_both']/stats['total_records']*100:.2f}%\",\n",
        "                \"both_elev_height\": f\"{stats['records_with_both_elev_height']/stats['total_records']*100:.2f}%\"\n",
        "            }\n",
        "        },\n",
        "        \"by_type_period\": stats['by_type_period'],\n",
        "        \"notes\": [\n",
        "            \"numpy_file and weather_file fields contain full absolute paths\",\n",
        "            \"Gage data uses prefix 'gage_' for files\",\n",
        "            \"HWM data uses prefix 'HWM_' for files\",\n",
        "            \"2016-2017 data uses embedding_1Y_later directory\",\n",
        "            \"2018+ data uses embedding_1Y_early directory\",\n",
        "            \"All elevation values are in meters\",\n",
        "            \"Optimized version with consolidated statistics\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Create output filename with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_file = os.path.join(output_dir, f\"flooding_dataset_optimized_{timestamp}.json\")\n",
        "\n",
        "print(f\"Generating JSON dataset...\")\n",
        "print(f\"Output file: {output_file}\")\n",
        "\n",
        "# Create dataset structure using pre-calculated statistics\n",
        "dataset = {\n",
        "    \"metadata\": create_dataset_metadata(stats, numpy_file_counts, weather_file_counts),\n",
        "    \"data\": all_data\n",
        "}\n",
        "\n",
        "# Write JSON file\n",
        "try:\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    file_size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
        "    print(f\"✓ JSON dataset generated successfully!\")\n",
        "    print(f\"  File: {output_file}\")\n",
        "    print(f\"  Size: {file_size_mb:.2f} MB\")\n",
        "    print(f\"  Records: {stats['total_records']:,}\")\n",
        "    print(f\"  Records with both files: {stats['records_with_both']:,} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
        "    print(f\"  Records with both elevations: {stats['records_with_both_elev_height']:,} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error generating JSON file: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Record Verification\n",
            "============================================================\n",
            "\n",
            "1. Sample Gage Record:\n",
            "  ID: 10001, Site: IAFAY24003\n",
            "  Coordinates: (42.95753, -91.62403)\n",
            "  Files exist: numpy=True, weather=True\n",
            "\n",
            "2. Sample HWM Record:\n",
            "  ID: 1, Site: IAFAY24003\n",
            "  Coordinates: (42.95753, -91.62403)\n",
            "  Files exist: numpy=True, weather=True\n",
            "\n",
            "============================================================\n",
            "Dataset Generation Summary\n",
            "============================================================\n",
            "✓ Total records processed: 6,891\n",
            "✓ Gage records: 4,246\n",
            "✓ HWM records: 2,645\n",
            "✓ Records with both files: 6,891 (100.0%)\n",
            "✓ Records with both elevations: 3,720 (54.0%)\n",
            "\n",
            "✓ JSON file generated: /u/wz53/alphaearth/Flooding_event_/Flood_dataset_example/flooding_dataset_optimized_20251020_233912.json\n",
            "✓ File size: 3.98 MB\n",
            "\n",
            "Dataset is ready for analysis! 🎉\n"
          ]
        }
      ],
      "source": [
        "def verify_sample_records(all_data):\n",
        "    \"\"\"Verify sample records and check file existence\"\"\"\n",
        "    print(\"Sample Record Verification\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Check a gage record\n",
        "    gage_samples = [r for r in all_data if r['data_type'] == 'gage' and r['numpy_file']]\n",
        "    if gage_samples:\n",
        "        sample = gage_samples[0]\n",
        "        print(f\"\\n1. Sample Gage Record:\")\n",
        "        print(f\"  ID: {sample['ID']}, Site: {sample['site_no']}\")\n",
        "        print(f\"  Coordinates: ({sample['latitude']}, {sample['longitude']})\")\n",
        "        print(f\"  Files exist: numpy={os.path.exists(sample['numpy_file'])}, weather={os.path.exists(sample['weather_file']) if sample['weather_file'] else False}\")\n",
        "    \n",
        "    # Check a HWM record\n",
        "    hwm_samples = [r for r in all_data if r['data_type'] == 'hwm' and r['numpy_file']]\n",
        "    if hwm_samples:\n",
        "        sample = hwm_samples[0]\n",
        "        print(f\"\\n2. Sample HWM Record:\")\n",
        "        print(f\"  ID: {sample['ID']}, Site: {sample['site_no']}\")\n",
        "        print(f\"  Coordinates: ({sample['latitude']}, {sample['longitude']})\")\n",
        "        print(f\"  Files exist: numpy={os.path.exists(sample['numpy_file'])}, weather={os.path.exists(sample['weather_file']) if sample['weather_file'] else False}\")\n",
        "\n",
        "# Verify sample records\n",
        "verify_sample_records(all_data)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Dataset Generation Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"✓ Total records processed: {stats['total_records']:,}\")\n",
        "print(f\"✓ Gage records: {stats['gage_records']:,}\")\n",
        "print(f\"✓ HWM records: {stats['hwm_records']:,}\")\n",
        "print(f\"✓ Records with both files: {stats['records_with_both']:,} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"✓ Records with both elevations: {stats['records_with_both_elev_height']:,} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n",
        "print(f\"\\n✓ JSON file generated: {output_file}\")\n",
        "print(f\"✓ File size: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "print(\"\\nDataset is ready for analysis! 🎉\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Filtered Version (Optional)\n",
        "\n",
        "Generate a filtered version containing only records with both elevation_m and height_above_gnd_m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating filtered version...\n",
            "============================================================\n",
            "Filtered records: 3720 / 6891\n",
            "  Gage: 1448, HWM: 2272\n",
            "\n",
            "============================================================\n",
            "✓ Filtered version generated!\n",
            "  File: /u/wz53/alphaearth/Flooding_event_/Flood_dataset_example/flooding_dataset_filtered_with_both_elevations_3720items.json\n",
            "  Size: 1.98 MB\n",
            "  Records: 3,720\n",
            "  Percentage: 54.0% of total\n"
          ]
        }
      ],
      "source": [
        "def generate_filtered_dataset(all_data, stats, output_dir, source_file):\n",
        "    \"\"\"Generate filtered dataset with both elevation values\"\"\"\n",
        "    print(\"Generating filtered version...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Filter records using pre-calculated stats\n",
        "    filtered_records = [r for r in all_data \n",
        "                        if r.get('elevation_m') is not None \n",
        "                        and r.get('height_above_gnd_m') is not None]\n",
        "    \n",
        "    # Calculate filtered statistics\n",
        "    gage_filtered = len([r for r in filtered_records if r['data_type'] == 'gage'])\n",
        "    hwm_filtered = len([r for r in filtered_records if r['data_type'] == 'hwm'])\n",
        "    \n",
        "    print(f\"Filtered records: {len(filtered_records)} / {stats['total_records']}\")\n",
        "    print(f\"  Gage: {gage_filtered}, HWM: {hwm_filtered}\")\n",
        "    \n",
        "    # Create filtered dataset\n",
        "    filtered_dataset = {\n",
        "        \"metadata\": {\n",
        "            \"description\": \"Filtered flooding event dataset with both elevation_m and height_above_gnd_m values (Optimized Version)\",\n",
        "            \"version\": \"5.0_filtered\",\n",
        "            \"generated_timestamp\": datetime.now().isoformat(),\n",
        "            \"source_file\": source_file,\n",
        "            \"filter_criteria\": \"Records with both elevation_m and height_above_gnd_m not null\",\n",
        "            \"original_total_records\": stats['total_records'],\n",
        "            \"filtered_total_records\": len(filtered_records),\n",
        "            \"total_records\": len(filtered_records),\n",
        "            \"gage_records\": gage_filtered,\n",
        "            \"hwm_records\": hwm_filtered,\n",
        "            \"notes\": [\n",
        "                f\"Filtered from {stats['total_records']} records to {len(filtered_records)} records\",\n",
        "                \"All records have both elevation_m and height_above_gnd_m values\",\n",
        "                \"Optimized version with consolidated statistics\"\n",
        "            ]\n",
        "        },\n",
        "        \"data\": filtered_records\n",
        "    }\n",
        "    \n",
        "    # Save filtered file\n",
        "    filtered_output = os.path.join(output_dir, f\"flooding_dataset_filtered_with_both_elevations_{len(filtered_records)}items.json\")\n",
        "    \n",
        "    with open(filtered_output, 'w', encoding='utf-8') as f:\n",
        "        json.dump(filtered_dataset, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    file_size_mb = os.path.getsize(filtered_output) / 1024 / 1024\n",
        "    \n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"✓ Filtered version generated!\")\n",
        "    print(f\"  File: {filtered_output}\")\n",
        "    print(f\"  Size: {file_size_mb:.2f} MB\")\n",
        "    print(f\"  Records: {len(filtered_records):,}\")\n",
        "    print(f\"  Percentage: {len(filtered_records)/stats['total_records']*100:.1f}% of total\")\n",
        "    \n",
        "    return filtered_output\n",
        "\n",
        "# Generate filtered version\n",
        "filtered_output = generate_filtered_dataset(all_data, stats, output_dir, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to Use the Generated Dataset\n",
            "============================================================\n",
            "\n",
            "# Load the JSON dataset\n",
            "import json\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "\n",
            "# Load JSON file\n",
            "with open('flooding_dataset_updated_TIMESTAMP.json', 'r') as f:\n",
            "    dataset = json.load(f)\n",
            "\n",
            "# Access metadata\n",
            "metadata = dataset['metadata']\n",
            "print(f\"Total records: {metadata['total_records']}\")\n",
            "print(f\"Gage records: {metadata['gage_records']}\")\n",
            "print(f\"HWM records: {metadata['hwm_records']}\")\n",
            "\n",
            "# Access data records\n",
            "data_records = dataset['data']\n",
            "\n",
            "# Filter records with both files\n",
            "complete_records = [r for r in data_records if r['numpy_file'] and r['weather_file']]\n",
            "print(f\"Complete records: {len(complete_records)}\")\n",
            "\n",
            "# Example: Load a specific record's data\n",
            "record = complete_records[0]\n",
            "print(f\"\\nRecord ID: {record['ID']}\")\n",
            "print(f\"Type: {record['data_type']}\")\n",
            "print(f\"Location: ({record['latitude']}, {record['longitude']})\")\n",
            "\n",
            "# Load numpy embedding\n",
            "embedding = np.load(record['numpy_file'])\n",
            "print(f\"Embedding shape: {embedding['image_data'].shape}\")\n",
            "\n",
            "# Load weather data\n",
            "weather_df = pd.read_csv(record['weather_file'])\n",
            "print(f\"Weather data shape: {weather_df.shape}\")\n",
            "print(f\"Weather columns: {list(weather_df.columns)}\")\n",
            "\n",
            "# Example: Batch processing\n",
            "for record in complete_records[:10]:  # Process first 10 records\n",
            "    # Load embedding\n",
            "    embedding = np.load(record['numpy_file'])\n",
            "    \n",
            "    # Load weather\n",
            "    weather = pd.read_csv(record['weather_file'])\n",
            "    \n",
            "    # Your processing logic here\n",
            "    pass\n",
            "\n",
            "\n",
            "Key Features:\n",
            "  • All file paths are absolute paths\n",
            "  • Easy to filter by data_type (gage/hwm) and period (2016_2017/2018_later)\n",
            "  • Metadata provides complete statistics and directory information\n",
            "  • Ready for machine learning and analysis workflows\n",
            "\n",
            "============================================================\n",
            "Dataset generation completed successfully! 🎉\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: How to load and use the generated JSON dataset\n",
        "print(\"How to Use the Generated Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "# Load the JSON dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load JSON file\n",
        "with open('flooding_dataset_updated_TIMESTAMP.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Access metadata\n",
        "metadata = dataset['metadata']\n",
        "print(f\"Total records: {metadata['total_records']}\")\n",
        "print(f\"Gage records: {metadata['gage_records']}\")\n",
        "print(f\"HWM records: {metadata['hwm_records']}\")\n",
        "\n",
        "# Access data records\n",
        "data_records = dataset['data']\n",
        "\n",
        "# Filter records with both files\n",
        "complete_records = [r for r in data_records if r['numpy_file'] and r['weather_file']]\n",
        "print(f\"Complete records: {len(complete_records)}\")\n",
        "\n",
        "# Example: Load a specific record's data\n",
        "record = complete_records[0]\n",
        "print(f\"\\\\nRecord ID: {record['ID']}\")\n",
        "print(f\"Type: {record['data_type']}\")\n",
        "print(f\"Location: ({record['latitude']}, {record['longitude']})\")\n",
        "\n",
        "# Load numpy embedding\n",
        "embedding = np.load(record['numpy_file'])\n",
        "print(f\"Embedding shape: {embedding['image_data'].shape}\")\n",
        "\n",
        "# Load weather data\n",
        "weather_df = pd.read_csv(record['weather_file'])\n",
        "print(f\"Weather data shape: {weather_df.shape}\")\n",
        "print(f\"Weather columns: {list(weather_df.columns)}\")\n",
        "\n",
        "# Example: Batch processing\n",
        "for record in complete_records[:10]:  # Process first 10 records\n",
        "    # Load embedding\n",
        "    embedding = np.load(record['numpy_file'])\n",
        "    \n",
        "    # Load weather\n",
        "    weather = pd.read_csv(record['weather_file'])\n",
        "    \n",
        "    # Your processing logic here\n",
        "    pass\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nKey Features:\")\n",
        "print(\"  • All file paths are absolute paths\")\n",
        "print(\"  • Easy to filter by data_type (gage/hwm) and period (2016_2017/2018_later)\")\n",
        "print(\"  • Metadata provides complete statistics and directory information\")\n",
        "print(\"  • Ready for machine learning and analysis workflows\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Dataset generation completed successfully! 🎉\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
