{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weather Data Download (First 3 Features - Multiple Time Windows)\n",
        "\n",
        "This notebook downloads weather data from Open-Meteo API for **the first 3 features from each CSV file** with multiple time window options. This provides complete weather data for a manageable subset of flooding events.\n",
        "\n",
        "## Features\n",
        "- Downloads weather data with multiple time window options:\n",
        "  - **24h**: 24-1 hours before peak flooding\n",
        "  - **96h**: 96-1 hours before peak flooding  \n",
        "  - **144h**: 144-121 hours before peak flooding\n",
        "- **Processes first 3 samples** from each input CSV file\n",
        "- Handles API rate limiting with retry mechanisms\n",
        "- Optimized parameter selection (removes null-heavy parameters)\n",
        "- Comprehensive logging and monitoring\n",
        "- Ready to run - no comments to uncomment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import signal\n",
        "import traceback\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "from collections import defaultdict\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API Rate Limiting Analysis\n",
        "\n",
        "### Open-Meteo API Rate Limits (Free Tier)\n",
        "- **Per minute**: 600 requests\n",
        "- **Per hour**: 5,000 requests  \n",
        "- **Per day**: 10,000 requests\n",
        "- **Concurrent requests**: Recommended < 4 to avoid connection rejection\n",
        "\n",
        "\n",
        "### Optimization Strategy\n",
        "1. **Reduce concurrency**: From 6 to 2 threads\n",
        "2. **Remove null-heavy parameters**: Eliminated 11 parameters with 100% null values\n",
        "3. **Implement retry mechanism**: Exponential backoff for rate limit errors\n",
        "4. **Add request intervals**: Prevent rapid successive requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Optimized Weather Data Downloader Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimizedWeatherDownloader:\n",
        "    \"\"\"\n",
        "    Optimized weather data downloader with rate limiting handling\n",
        "    Downloads first 3 features from each CSV file\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_output_dir, max_workers=2, max_samples=3, time_window=\"24h\", force_redownload=False):\n",
        "        self.base_output_dir = base_output_dir\n",
        "        self.max_workers = max_workers\n",
        "        self.max_samples = max_samples  # Process first 3 samples from each file\n",
        "        self.time_window = time_window  # Time window: \"24h\", \"96h\", \"144h\"\n",
        "        self.force_redownload = force_redownload  # Force redownload even if files exist\n",
        "        self.completed_samples = set()\n",
        "        self.failed_samples = set()\n",
        "        self.lock = threading.Lock()\n",
        "        self.session_cache = {}\n",
        "        \n",
        "        # Complete weather parameters (32 variables as per README)\n",
        "        self.weather_params = [\n",
        "            # Temperature related\n",
        "            'temperature_2m', 'apparent_temperature', 'dewpoint_2m',\n",
        "            'soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm',\n",
        "            \n",
        "            # Precipitation related\n",
        "            'precipitation', 'rain', 'snowfall',\n",
        "            \n",
        "            # Humidity related\n",
        "            'relative_humidity_2m', 'vapour_pressure_deficit',\n",
        "            'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', \n",
        "            'soil_moisture_9_27cm', 'soil_moisture_27_81cm',\n",
        "            \n",
        "            # Pressure related\n",
        "            'pressure_msl', 'surface_pressure',\n",
        "            \n",
        "            # Cloud cover related\n",
        "            'cloud_cover', 'cloud_cover_low', 'cloud_cover_mid', 'cloud_cover_high',\n",
        "            \n",
        "            # Wind related\n",
        "            'wind_speed_10m', 'wind_speed_100m', 'wind_direction_10m', 'wind_direction_100m', 'wind_gusts_10m',\n",
        "            \n",
        "            # Other\n",
        "            'weather_code', 'visibility', 'evapotranspiration', 'et0_fao_evapotranspiration'\n",
        "        ]\n",
        "        \n",
        "        # No excluded parameters - using complete set as per README\n",
        "        self.excluded_params = []\n",
        "        \n",
        "        # Create base directory\n",
        "        os.makedirs(base_output_dir, exist_ok=True)\n",
        "        self.setup_logging()\n",
        "    \n",
        "    def get_output_dir_for_sample(self, row, csv_file):\n",
        "        \"\"\"Get output directory based on data source and year (as per README)\"\"\"\n",
        "        # Determine data source\n",
        "        csv_filename = os.path.basename(csv_file)\n",
        "        if 'Unified_Peak_Data' in csv_filename:\n",
        "            data_source = 'gage'\n",
        "        elif 'matched_records' in csv_filename:\n",
        "            data_source = 'hwm'\n",
        "        else:\n",
        "            data_source = 'unknown'\n",
        "        \n",
        "        # Determine year from peak_date\n",
        "        peak_date = None\n",
        "        if 'peak_date' in row:\n",
        "            peak_date = pd.to_datetime(row['peak_date'])\n",
        "        elif 'matched_peak_date' in row:\n",
        "            peak_date = pd.to_datetime(row['matched_peak_date'])\n",
        "        \n",
        "        if peak_date is None:\n",
        "            base_dir_name = f\"unknown_{self.time_window}\"\n",
        "        else:\n",
        "            year = peak_date.year\n",
        "            \n",
        "            # Create directory name based on data source and year\n",
        "            if data_source == 'gage':\n",
        "                if year in [2016, 2017]:\n",
        "                    base_dir_name = f\"gage_2016_2017_{self.time_window}\"\n",
        "                else:\n",
        "                    base_dir_name = f\"gage_2018_later_{self.time_window}\"\n",
        "            elif data_source == 'hwm':\n",
        "                if year in [2016, 2017]:\n",
        "                    base_dir_name = f\"hwm_2016_2017_{self.time_window}\"\n",
        "                else:\n",
        "                    base_dir_name = f\"hwm_2018_later_{self.time_window}\"\n",
        "            else:\n",
        "                base_dir_name = f\"unknown_{self.time_window}\"\n",
        "        \n",
        "        # Add suffix for force redownload\n",
        "        if self.force_redownload:\n",
        "            base_dir_name = f\"{base_dir_name}_redownload\"\n",
        "        \n",
        "        output_dir = os.path.join(self.base_output_dir, base_dir_name)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        return output_dir\n",
        "    \n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        log_dir = os.path.join(os.path.dirname(self.base_output_dir), 'logs')\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        \n",
        "        log_file = os.path.join(log_dir, f\"weather_download_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        \n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file),\n",
        "                logging.StreamHandler(sys.stdout)\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "    \n",
        "    def get_session(self):\n",
        "        \"\"\"Get or create HTTP session for connection reuse\"\"\"\n",
        "        thread_id = threading.current_thread().ident\n",
        "        if thread_id not in self.session_cache:\n",
        "            self.session_cache[thread_id] = requests.Session()\n",
        "        return self.session_cache[thread_id]\n",
        "    \n",
        "    def build_api_url(self, lat, lon, start_date, end_date):\n",
        "        \"\"\"Build Open-Meteo API URL with optimized parameters\"\"\"\n",
        "        params_str = ','.join(self.weather_params)\n",
        "        return (\n",
        "            f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
        "            f\"latitude={lat}&longitude={lon}&start_date={start_date}&end_date={end_date}\"\n",
        "            f\"&hourly={params_str}&timezone=UTC\"\n",
        "        )\n",
        "    \n",
        "    def fetch_weather_data(self, sample_id, lat, lon, peak_date, output_dir, max_retries=3):\n",
        "        \"\"\"\n",
        "        Fetch weather data with retry mechanism for rate limiting\n",
        "        \n",
        "        Args:\n",
        "            sample_id: Unique identifier for the sample\n",
        "            lat, lon: Coordinates\n",
        "            peak_date: Peak flooding date\n",
        "            max_retries: Maximum number of retry attempts\n",
        "        \"\"\"\n",
        "        # Convert peak_date to UTC\n",
        "        if peak_date.tzinfo is None:\n",
        "            peak_date_utc = peak_date.replace(tzinfo=pytz.UTC)\n",
        "        else:\n",
        "            peak_date_utc = peak_date.astimezone(pytz.UTC)\n",
        "        \n",
        "        # Calculate time window before peak based on configuration\n",
        "        # Note: 96h window now provides 96 hours of data, others provide 24 hours\n",
        "        if self.time_window == \"24h\":\n",
        "            # 24h window: 24 hours before peak to 1 hour before peak (24 hours total)\n",
        "            start_time = peak_date_utc - timedelta(hours=24)\n",
        "            end_time = peak_date_utc - timedelta(hours=1)\n",
        "        elif self.time_window == \"96h\":\n",
        "            # 96h window: 96 hours before peak to 1 hour before peak (96 hours total)\n",
        "            start_time = peak_date_utc - timedelta(hours=96)\n",
        "            end_time = peak_date_utc - timedelta(hours=1)\n",
        "        elif self.time_window == \"144h\":\n",
        "            # 144h window: 144 hours before peak to 121 hours before peak (24 hours total)\n",
        "            start_time = peak_date_utc - timedelta(hours=144)\n",
        "            end_time = peak_date_utc - timedelta(hours=121)\n",
        "        else:\n",
        "            # Default to 24h\n",
        "            start_time = peak_date_utc - timedelta(hours=24)\n",
        "            end_time = peak_date_utc - timedelta(hours=1)\n",
        "        \n",
        "        # Use date format for API request (Open-Meteo only supports date format)\n",
        "        start_str = start_time.strftime(\"%Y-%m-%d\")\n",
        "        end_str = end_time.strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "        url = self.build_api_url(lat, lon, start_str, end_str)\n",
        "        session = self.get_session()\n",
        "        \n",
        "        # Retry mechanism with exponential backoff\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                if attempt > 0:\n",
        "                    wait_time = 2 ** attempt  # Exponential backoff\n",
        "                    self.logger.warning(f\"Retrying after {wait_time}s, attempt {attempt+1}/{max_retries}\")\n",
        "                    time.sleep(wait_time)\n",
        "                \n",
        "                response = session.get(url, timeout=60)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                \n",
        "                if \"hourly\" not in data or \"time\" not in data[\"hourly\"]:\n",
        "                    raise ValueError(\"No hourly data available\")\n",
        "                \n",
        "                return self.process_weather_data(data, sample_id, lat, lon, peak_date, output_dir)\n",
        "                \n",
        "            except requests.exceptions.HTTPError as e:\n",
        "                if e.response.status_code == 429:  # Rate limited\n",
        "                    if attempt < max_retries - 1:\n",
        "                        wait_time = 60 * (attempt + 1)  # Wait 1, 2, 3 minutes\n",
        "                        self.logger.warning(f\"Rate limited, waiting {wait_time}s before retry\")\n",
        "                        time.sleep(wait_time)\n",
        "                        continue\n",
        "                    else:\n",
        "                        self.logger.error(f\"Rate limit exceeded after {max_retries} attempts\")\n",
        "                        raise\n",
        "                else:\n",
        "                    self.logger.error(f\"HTTP error: {e}\")\n",
        "                    raise\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    self.logger.warning(f\"Request failed, retrying: {e}\")\n",
        "                    time.sleep(5)\n",
        "                    continue\n",
        "                else:\n",
        "                    self.logger.error(f\"Request failed after {max_retries} attempts: {e}\")\n",
        "                    raise\n",
        "    \n",
        "    def process_weather_data(self, data, sample_id, lat, lon, peak_date, output_dir):\n",
        "        \"\"\"Process and save weather data with time window filtering\"\"\"\n",
        "        df = pd.DataFrame(data[\"hourly\"])\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        df['time'] = pd.to_datetime(df['time']).dt.tz_localize(None)\n",
        "        \n",
        "        # Calculate time window boundaries\n",
        "        if peak_date.tzinfo is None:\n",
        "            peak_date_utc = peak_date.replace(tzinfo=pytz.UTC)\n",
        "        else:\n",
        "            peak_date_utc = peak_date.astimezone(pytz.UTC)\n",
        "        \n",
        "        # Calculate time window based on configuration (must match fetch_weather_data)\n",
        "        if self.time_window == \"24h\":\n",
        "            # 24h window: 24 hours before peak to 1 hour before peak (24 hours total)\n",
        "            start_time = peak_date_utc - timedelta(hours=24)\n",
        "            end_time = peak_date_utc - timedelta(hours=1)\n",
        "        elif self.time_window == \"96h\":\n",
        "            # 96h window: 96 hours before peak to 1 hour before peak (96 hours total)\n",
        "            start_time = peak_date_utc - timedelta(hours=96)\n",
        "            end_time = peak_date_utc - timedelta(hours=1)\n",
        "        elif self.time_window == \"144h\":\n",
        "            # 144h window: 144 hours before peak to 121 hours before peak (24 hours total)\n",
        "            start_time = peak_date_utc - timedelta(hours=144)\n",
        "            end_time = peak_date_utc - timedelta(hours=121)\n",
        "        else:\n",
        "            # Default to 24h\n",
        "            start_time = peak_date_utc - timedelta(hours=24)\n",
        "            end_time = peak_date_utc - timedelta(hours=1)\n",
        "        \n",
        "        # Convert to naive datetime for comparison\n",
        "        start_time_naive = start_time.replace(tzinfo=None)\n",
        "        end_time_naive = end_time.replace(tzinfo=None)\n",
        "        \n",
        "        # Filter data to exact time window\n",
        "        mask = (df['time'] >= start_time_naive) & (df['time'] <= end_time_naive)\n",
        "        df = df[mask].copy()\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            self.logger.warning(f\"No data in time window for {sample_id}\")\n",
        "            return 0\n",
        "        \n",
        "        # Add metadata\n",
        "        df['sample_id'] = sample_id\n",
        "        df['latitude'] = lat\n",
        "        df['longitude'] = lon\n",
        "        df['peak_date'] = peak_date.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        df['hours_before_peak'] = (peak_date - df['time']).dt.total_seconds() / 3600\n",
        "        \n",
        "        # Save to CSV\n",
        "        output_file = os.path.join(output_dir, f\"{sample_id}.csv\")\n",
        "        df.to_csv(output_file, index=False)\n",
        "        \n",
        "        return len(df)\n",
        "    \n",
        "    def process_sample(self, sample_data):\n",
        "        \"\"\"Process a single sample\"\"\"\n",
        "        sample_id, row, csv_file = sample_data\n",
        "        \n",
        "        try:\n",
        "            # Get output directory for this sample\n",
        "            output_dir = self.get_output_dir_for_sample(row, csv_file)\n",
        "            \n",
        "            # Extract coordinates\n",
        "            if 'latitude' in row and 'longitude' in row:\n",
        "                lat, lon = row['latitude'], row['longitude']\n",
        "            elif 'latitude_dd' in row and 'longitude_dd' in row:\n",
        "                lat, lon = row['latitude_dd'], row['longitude_dd']\n",
        "            else:\n",
        "                raise ValueError(\"No coordinates found\")\n",
        "            \n",
        "            # Extract peak date\n",
        "            if 'peak_date' in row:\n",
        "                peak_date = pd.to_datetime(row['peak_date'])\n",
        "            elif 'matched_peak_date' in row:\n",
        "                peak_date = pd.to_datetime(row['matched_peak_date'])\n",
        "            else:\n",
        "                raise ValueError(\"No peak date found\")\n",
        "            \n",
        "            # Check if file already exists (unless force redownload is enabled)\n",
        "            output_file = os.path.join(output_dir, f\"{sample_id}.csv\")\n",
        "            if not self.force_redownload and os.path.exists(output_file):\n",
        "                return sample_id, \"skipped\", 0\n",
        "            \n",
        "            # Fetch weather data\n",
        "            record_count = self.fetch_weather_data(sample_id, lat, lon, peak_date, output_dir)\n",
        "            \n",
        "            with self.lock:\n",
        "                self.completed_samples.add(sample_id)\n",
        "            \n",
        "            return sample_id, \"completed\", record_count\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to process {sample_id}: {e}\")\n",
        "            with self.lock:\n",
        "                self.failed_samples.add(sample_id)\n",
        "            return sample_id, \"failed\", 0\n",
        "    \n",
        "    def download_all_samples(self, csv_files):\n",
        "        \"\"\"Download weather data for first 3 samples from each file\"\"\"\n",
        "        all_samples = []\n",
        "        \n",
        "        # Load samples from CSV files\n",
        "        for csv_file in csv_files:\n",
        "            if os.path.exists(csv_file):\n",
        "                self.logger.info(f\"Loading {csv_file}\")\n",
        "                df = pd.read_csv(csv_file, low_memory=False)\n",
        "                total_rows = len(df)\n",
        "                \n",
        "                # Process first 3 samples ONLY\n",
        "                df_limited = df.head(self.max_samples)\n",
        "                self.logger.info(f\"  Processing first {len(df_limited)} samples (from {total_rows} total)\")\n",
        "                self.logger.info(f\"  Limited to max_samples={self.max_samples} as configured\")\n",
        "                \n",
        "                # Verify we're only processing the first 3\n",
        "                if len(df_limited) > self.max_samples:\n",
        "                    self.logger.warning(f\"  WARNING: Processing {len(df_limited)} samples, expected max {self.max_samples}\")\n",
        "                \n",
        "                for idx, row in df_limited.iterrows():\n",
        "                    sample_id = self.create_sample_id(row, csv_file, idx)\n",
        "                    all_samples.append((sample_id, row, csv_file))\n",
        "                    self.logger.debug(f\"    Added sample {idx+1}/{len(df_limited)}: {sample_id}\")\n",
        "        \n",
        "        self.logger.info(f\"Total samples to process: {len(all_samples)} (first {self.max_samples} from each file)\")\n",
        "        \n",
        "        # Verify configuration\n",
        "        expected_samples = len(csv_files) * self.max_samples\n",
        "        if len(all_samples) != expected_samples:\n",
        "            self.logger.warning(f\"Expected {expected_samples} samples, but got {len(all_samples)}\")\n",
        "        \n",
        "        # Process samples with thread pool\n",
        "        completed_count = 0\n",
        "        failed_count = 0\n",
        "        skipped_count = 0\n",
        "        \n",
        "        self.logger.info(f\"Starting to process {len(all_samples)} samples...\")\n",
        "        \n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            future_to_sample = {executor.submit(self.process_sample, sample): sample[0] \n",
        "                              for sample in all_samples}\n",
        "            \n",
        "            for future in as_completed(future_to_sample):\n",
        "                sample_id = future_to_sample[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    sample_id, status, count = result\n",
        "                    \n",
        "                    if status == \"completed\":\n",
        "                        completed_count += 1\n",
        "                        self.logger.info(f\"Completed {sample_id}: {count} records\")\n",
        "                    elif status == \"failed\":\n",
        "                        failed_count += 1\n",
        "                        self.logger.error(f\"Failed {sample_id}\")\n",
        "                    elif status == \"skipped\":\n",
        "                        skipped_count += 1\n",
        "                        self.logger.info(f\"Skipped {sample_id}: file already exists\")\n",
        "                    \n",
        "                    # Progress update every 5 samples (reduced from 20 for better visibility)\n",
        "                    total_processed = completed_count + failed_count + skipped_count\n",
        "                    if total_processed % 5 == 0 and total_processed > 0:\n",
        "                        self.logger.info(f\"Progress: {completed_count} completed, {failed_count} failed, {skipped_count} skipped\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Exception processing {sample_id}: {e}\")\n",
        "                    failed_count += 1\n",
        "        \n",
        "        # Cleanup\n",
        "        for session in self.session_cache.values():\n",
        "            session.close()\n",
        "        \n",
        "        self.logger.info(f\"Download complete: {completed_count} successful, {failed_count} failed, {skipped_count} skipped\")\n",
        "        return completed_count, failed_count\n",
        "    \n",
        "    def create_sample_id(self, row, csv_file, idx):\n",
        "        \"\"\"Create unique sample ID from row data (using ID field as per README)\"\"\"\n",
        "        # Use ID field if available (as per README specification)\n",
        "        if 'ID' in row and pd.notna(row['ID']):\n",
        "            return str(int(row['ID']))\n",
        "        \n",
        "        # Fallback to original logic if ID not available\n",
        "        # Handle Unified_Peak_Data file\n",
        "        if 'site_no' in row and 'event' in row:\n",
        "            site_no = str(row['site_no'])\n",
        "            event_name = str(row['event']).replace(' ', '_')\n",
        "            return f\"{site_no}_{event_name}\"\n",
        "        \n",
        "        # Handle HWMs_with_peaktime file\n",
        "        elif 'site_no' in row and 'eventName' in row:\n",
        "            site_no = str(row['site_no'])\n",
        "            event_name = str(row['eventName']).replace(' ', '_')\n",
        "            if event_name == 'nan':\n",
        "                event_name = 'unknown_event'\n",
        "            return f\"{site_no}_{event_name}\"\n",
        "        \n",
        "        # Final fallback\n",
        "        if 'site_no' in row:\n",
        "            return f\"{row['site_no']}_unknown_event_{idx}\"\n",
        "        else:\n",
        "            return f\"sample_{os.path.basename(csv_file).split('.')[0]}_{idx}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Usage Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting weather data download (LIMITED to first 3 features from each file, 96h window)...\n",
            "======================================================================\n",
            "Time window: 96h\n",
            " Base output directory: /u/wz53/alphaearth/Flooding_event_/Flood_dataset\n",
            " Max workers: 2\n",
            " Max samples per file: 3 (LIMITED)\n",
            " Force redownload: True\n",
            "  Weather parameters: 32 (complete set as per README)\n",
            " Directory structure: gage_2016_2017_96h_redownload, gage_2018_later_96h_redownload, hwm_2016_2017_96h_redownload, hwm_2018_later_96h_redownload\n",
            "======================================================================\n",
            "2025-10-21 14:38:50,255 - INFO - Loading /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv\n",
            "2025-10-21 14:38:50,267 - INFO -   Processing first 3 samples (from 1949 total)\n",
            "2025-10-21 14:38:50,268 - INFO -   Limited to max_samples=3 as configured\n",
            "2025-10-21 14:38:50,275 - INFO - Loading /u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-21 14:38:50,297 - INFO -   Processing first 3 samples (from 2297 total)\n",
            "2025-10-21 14:38:50,299 - INFO -   Limited to max_samples=3 as configured\n",
            "2025-10-21 14:38:50,301 - INFO - Loading /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv\n",
            "2025-10-21 14:38:50,315 - INFO -   Processing first 3 samples (from 1947 total)\n",
            "2025-10-21 14:38:50,316 - INFO -   Limited to max_samples=3 as configured\n",
            "2025-10-21 14:38:50,319 - INFO - Loading /u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv\n",
            "2025-10-21 14:38:50,328 - INFO -   Processing first 3 samples (from 698 total)\n",
            "2025-10-21 14:38:50,333 - INFO -   Limited to max_samples=3 as configured\n",
            "2025-10-21 14:38:50,340 - INFO - Total samples to process: 12 (first 3 from each file)\n",
            "2025-10-21 14:38:50,350 - INFO - Starting to process 12 samples...\n",
            "2025-10-21 14:38:50,907 - INFO - Completed 10001: 96 records\n",
            "2025-10-21 14:38:50,922 - INFO - Completed 10002: 96 records\n",
            "2025-10-21 14:38:51,071 - INFO - Completed 10003: 96 records\n",
            "2025-10-21 14:38:51,117 - INFO - Completed 12002: 96 records\n",
            "2025-10-21 14:38:51,227 - INFO - Completed 12003: 96 records\n",
            "2025-10-21 14:38:51,227 - INFO - Progress: 5 completed, 0 failed, 0 skipped\n",
            "2025-10-21 14:38:51,290 - INFO - Completed 12004: 96 records\n",
            "2025-10-21 14:38:51,401 - INFO - Completed 1: 96 records\n",
            "2025-10-21 14:38:51,446 - INFO - Completed 2: 96 records\n",
            "2025-10-21 14:38:51,577 - INFO - Completed 3: 95 records\n",
            "2025-10-21 14:38:51,622 - INFO - Completed 1949: 95 records\n",
            "2025-10-21 14:38:51,622 - INFO - Progress: 10 completed, 0 failed, 0 skipped\n",
            "2025-10-21 14:38:51,757 - INFO - Completed 1950: 95 records\n",
            "2025-10-21 14:38:51,805 - INFO - Completed 1951: 95 records\n",
            "2025-10-21 14:38:51,806 - INFO - Download complete: 12 successful, 0 failed, 0 skipped\n",
            "\n",
            "Download Results: 12 successful, 0 failed\n",
            "Note: Check the logs above for detailed progress including skipped files\n"
          ]
        }
      ],
      "source": [
        "# Initialize downloader (First 3 features from each file)\n",
        "# Time window options: \"24h\", \"96h\", \"144h\"\n",
        "time_window = \"96h\"  # Changed to 96h to download 96 hours before peak\n",
        "base_output_dir = '/u/wz53/alphaearth/Flooding_event_/Flood_dataset'\n",
        "force_redownload = True  # Set to True to force redownload even if files exist\n",
        "\n",
        "# IMPORTANT: max_samples=3 ensures only first 3 features from each CSV file\n",
        "downloader = OptimizedWeatherDownloader(\n",
        "    base_output_dir=base_output_dir, \n",
        "    max_workers=2, \n",
        "    max_samples=3,  # Only process first 3 samples from each CSV file\n",
        "    time_window=time_window,\n",
        "    force_redownload=force_redownload  # Force redownload to new directories\n",
        ")\n",
        "\n",
        "# Define input CSV files\n",
        "csv_files = [\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv',\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv',\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv',\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv'\n",
        "]\n",
        "\n",
        "# Start download process\n",
        "print(\"Starting weather data download (LIMITED to first 3 features from each file, 96h window)...\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Time window: {time_window}\")\n",
        "print(f\" Base output directory: {base_output_dir}\")\n",
        "print(f\" Max workers: {downloader.max_workers}\")\n",
        "print(f\" Max samples per file: {downloader.max_samples} (LIMITED)\")\n",
        "print(f\" Force redownload: {force_redownload}\")\n",
        "print(f\"  Weather parameters: {len(downloader.weather_params)} (complete set as per README)\")\n",
        "if force_redownload:\n",
        "    print(f\" Directory structure: gage_2016_2017_{time_window}_redownload, gage_2018_later_{time_window}_redownload, hwm_2016_2017_{time_window}_redownload, hwm_2018_later_{time_window}_redownload\")\n",
        "else:\n",
        "    print(f\" Directory structure: gage_2016_2017_{time_window}, gage_2018_later_{time_window}, hwm_2016_2017_{time_window}, hwm_2018_later_{time_window}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Run download\n",
        "completed, failed = downloader.download_all_samples(csv_files)\n",
        "print(f\"\\nDownload Results: {completed} successful, {failed} failed\")\n",
        "print(\"Note: Check the logs above for detailed progress including skipped files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Download Results Summary\n",
            "======================================================================\n",
            " Expected output: 12 weather data files (3 from each of 4 CSV files)\n",
            " Time window: 96h\n",
            " Sample limitation: FIRST 3 FEATURES ONLY from each CSV file\n",
            " Each file contains 23 hours of weather data (96-72 hours before peak)\n",
            " Files saved in organized directories:\n",
            "  - gage_2016_2017_96h/\n",
            "  - gage_2018_later_96h/\n",
            "  - hwm_2016_2017_96h/\n",
            "  - hwm_2018_later_96h/\n",
            "  Each file contains 32 weather parameters\n",
            "======================================================================\n",
            " Download completed! Check the organized directories for your weather data files.\n",
            " Note: Only the first 3 features from each CSV file were processed.\n"
          ]
        }
      ],
      "source": [
        "# Check download results\n",
        "print(\" Download Results Summary\")\n",
        "print(\"=\" * 70)\n",
        "print(f\" Expected output: 12 weather data files (3 from each of 4 CSV files)\")\n",
        "print(f\" Time window: {time_window}\")\n",
        "print(f\" Sample limitation: FIRST 3 FEATURES ONLY from each CSV file\")\n",
        "if time_window == \"24h\":\n",
        "    print(\" Each file contains 23 hours of weather data (24-1 hours before peak flooding)\")\n",
        "elif time_window == \"96h\":\n",
        "    print(\" Each file contains 23 hours of weather data (96-72 hours before peak)\")\n",
        "elif time_window == \"144h\":\n",
        "    print(\" Each file contains 23 hours of weather data (144-121 hours before peak)\")\n",
        "print(f\" Files saved in organized directories:\")\n",
        "print(f\"  - gage_2016_2017_{time_window}/\")\n",
        "print(f\"  - gage_2018_later_{time_window}/\")\n",
        "print(f\"  - hwm_2016_2017_{time_window}/\")\n",
        "print(f\"  - hwm_2018_later_{time_window}/\")\n",
        "print(f\"  Each file contains {len(downloader.weather_params)} weather parameters\")\n",
        "print(\"=\" * 70)\n",
        "print(\" Download completed! Check the organized directories for your weather data files.\")\n",
        "print(\" Note: Only the first 3 features from each CSV file were processed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 96-Hour Time Window Configuration\n",
        "\n",
        "###  Time Window Settings\n",
        "- **Current Setting**: 96-hour time window\n",
        "- **Data Range**: 96 hours before peak_date to 1 hour before peak_date\n",
        "- **Data Duration**: 96 hours of complete data\n",
        "- **Purpose**: Obtain complete weather background data for 96 hours before flood events\n",
        "\n",
        "###  Expected Results\n",
        "- **File Count**: 12 CSV files (first 3 features from each CSV file)\n",
        "- **Each File**: 96 rows of data (one record per hour)\n",
        "- **Weather Parameters**: 32 complete meteorological variables\n",
        "- **Directory Structure**: Organized by data source and year, with _redownload suffix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 96h Time Window Update Description\n",
        "\n",
        "###  Important Changes\n",
        "**96h time window has been updated to obtain complete 96-hour data**\n",
        "\n",
        "#### Before:\n",
        "- **Time Range**: 96 hours before peak_date to 73 hours before peak_date\n",
        "- **Data Duration**: 24 hours of data\n",
        "- **Record Count**: 24 records\n",
        "\n",
        "#### After:\n",
        "- **Time Range**: 96 hours before peak_date to 1 hour before peak_date  \n",
        "- **Data Duration**: 96 hours of data\n",
        "- **Record Count**: 96 records\n",
        "\n",
        "###  New Expected Results\n",
        "- **Each File**: 96 rows of data (one record per hour)\n",
        "- **Time Span**: Complete 96-hour continuous data\n",
        "- **Data Purpose**: Provide complete weather background for 96 hours before flood events\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extract 24-Hour Data Before Peak\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24-hour data extraction functions defined:\n",
            "- extract_24h_before_peak(): Extract 1-24 hours before peak\n",
            "- process_weather_file_to_24h(): Process file and save 24h data\n",
            "\n",
            "Usage example:\n",
            "df_24h = extract_24h_before_peak(df, peak_date)\n",
            "process_weather_file_to_24h('input.csv', 'output_24h.csv')\n"
          ]
        }
      ],
      "source": [
        "def extract_24h_before_peak(df, peak_date):\n",
        "    \"\"\"\n",
        "    Extract exactly 24 hours of data before peak from weather DataFrame\n",
        "    \n",
        "    Args:\n",
        "        df: Weather data DataFrame with 'time' column\n",
        "        peak_date: Peak flooding datetime\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with 23-24 rows (1-24 hours before peak)\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['time'] = pd.to_datetime(df_copy['time'])\n",
        "    \n",
        "    # Ensure peak_date is datetime\n",
        "    if isinstance(peak_date, str):\n",
        "        peak_date = pd.to_datetime(peak_date)\n",
        "    \n",
        "    # Calculate hours before peak\n",
        "    time_diff_hours = (peak_date - df_copy['time']).dt.total_seconds() / 3600\n",
        "    \n",
        "    # Filter: 1-24 hours before peak\n",
        "    mask = (time_diff_hours >= 1) & (time_diff_hours <= 24)\n",
        "    df_24h = df_copy[mask].copy()\n",
        "    \n",
        "    # Sort by time\n",
        "    df_24h = df_24h.sort_values('time')\n",
        "    \n",
        "    # Add hours_before_peak column\n",
        "    df_24h['hours_before_peak'] = time_diff_hours[mask]\n",
        "    \n",
        "    return df_24h\n",
        "\n",
        "def process_weather_file_to_24h(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process a weather file and extract 24h before peak\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "    \n",
        "    # Get peak_date\n",
        "    peak_date = pd.to_datetime(df['peak_date'].iloc[0])\n",
        "    \n",
        "    # Extract 24h data\n",
        "    df_24h = extract_24h_before_peak(df, peak_date)\n",
        "    \n",
        "    print(f\"Original: {len(df)} rows -> Extracted: {len(df_24h)} rows\")\n",
        "    print(f\"Time range: {df_24h['time'].min()} to {df_24h['time'].max()}\")\n",
        "    print(f\"Hours before peak: {df_24h['hours_before_peak'].min():.1f} to {df_24h['hours_before_peak'].max():.1f}\")\n",
        "    \n",
        "    # Save\n",
        "    df_24h.to_csv(output_file, index=False)\n",
        "    \n",
        "    return df_24h\n",
        "\n",
        "# Example usage\n",
        "print(\"24-hour data extraction functions defined:\")\n",
        "print(\"- extract_24h_before_peak(): Extract 1-24 hours before peak\")\n",
        "print(\"- process_weather_file_to_24h(): Process file and save 24h data\")\n",
        "print()\n",
        "print(\"Usage example:\")\n",
        "print(\"df_24h = extract_24h_before_peak(df, peak_date)\")\n",
        "print(\"process_weather_file_to_24h('input.csv', 'output_24h.csv')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing downloaded weather files to extract 24-hour data...\n",
            "============================================================\n",
            "No weather files found in input directory\n",
            "Please run the download first (Cell 7)\n"
          ]
        }
      ],
      "source": [
        "# Example: Process downloaded weather files to extract 24h data\n",
        "print(\"Processing downloaded weather files to extract 24-hour data...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Set up directories (adjust based on your time window)\n",
        "time_window = \"24h\"  # Should match the time window used in Cell 7\n",
        "input_dir = f'/u/wz53/alphaearth/Flooding_event_/Flood_dataset/weather_first_3_features_{time_window}'\n",
        "output_dir = f'/u/wz53/alphaearth/Flooding_event_/Flood_dataset/weather_24h_extracted_{time_window}'\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process all downloaded weather files\n",
        "if os.path.exists(input_dir):\n",
        "    weather_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
        "    \n",
        "    if weather_files:\n",
        "        print(f\"Found {len(weather_files)} weather files to process\")\n",
        "        print()\n",
        "        \n",
        "        for i, weather_file in enumerate(weather_files, 1):\n",
        "            input_path = os.path.join(input_dir, weather_file)\n",
        "            output_path = os.path.join(output_dir, f\"24h_{weather_file}\")\n",
        "            \n",
        "            print(f\"Processing {i}/{len(weather_files)}: {weather_file}\")\n",
        "            \n",
        "            try:\n",
        "                # Extract 24h data\n",
        "                df_24h = process_weather_file_to_24h(input_path, output_path)\n",
        "                print(f\"✓ Saved: {output_path}\")\n",
        "                print()\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error processing {weather_file}: {e}\")\n",
        "                print()\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(\"24-hour data extraction completed!\")\n",
        "        print(f\"Input directory: {input_dir}\")\n",
        "        print(f\"Output directory: {output_dir}\")\n",
        "        print(f\"Processed files: {len(weather_files)}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"No weather files found in input directory\")\n",
        "        print(\"Please run the download first (Cell 7)\")\n",
        "        \n",
        "else:\n",
        "    print(\"Input directory does not exist\")\n",
        "    print(\"Please run the download first (Cell 7)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Processing Summary\n",
        "\n",
        "### First 3 Features Processing Pipeline\n",
        "\n",
        "This notebook downloads weather data for **the first 3 features from each CSV file** with multiple time window options:\n",
        "\n",
        "#### Key Features\n",
        "- **Multiple Time Windows**: Choose from 24h, 96h, or 144h before peak flooding\n",
        "- **Limited Processing**: Processes first 3 samples from each CSV file (12 total)\n",
        "- **API Optimization**: Uses optimized parameters with rate limiting handling\n",
        "- **Quality Assurance**: Zero null values, complete weather parameters\n",
        "- **Flexible Extraction**: Extract data from different time periods before peak\n",
        "- **Ready to Run**: No comments to uncomment, direct execution\n",
        "\n",
        "#### Processing Steps\n",
        "1. **Download**: Download weather data for first 3 features from each CSV file\n",
        "2. **Extract**: Extract exactly 24 hours before peak flooding event\n",
        "3. **Save**: Save processed 24-hour data to separate directory\n",
        "\n",
        "#### Expected Output\n",
        "- **12 weather data files** (3 from each of 4 CSV files)\n",
        "- **12 extracted 24-hour files** (processed from downloaded files)\n",
        "- **24 hours of data** from selected time window before peak flooding\n",
        "- **22 weather parameters** per file\n",
        "- **Complete metadata** (coordinates, peak date, hours before peak)\n",
        "- **Time window specific directories** for organized data storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Quick Start Example\n",
        "\n",
        "### Run the Complete Pipeline\n",
        "\n",
        "To run the complete weather data download and extraction pipeline:\n",
        "\n",
        "1. **Choose Time Window**: Set `time_window` variable in Cell 7 to \"24h\", \"96h\", or \"144h\"\n",
        "2. **Execute Cell 7**: Download weather data for first 3 features from each CSV file\n",
        "3. **Execute Cell 11**: Extract 24-hour data from the selected time window\n",
        "4. **Check Results**: Verify files in time window specific directories\n",
        "\n",
        "### Time Window Options\n",
        "- **24h**: Peak flooding 24-1 hours before (immediate pre-flood conditions)\n",
        "- **96h**: Peak flooding 96-72 hours before (3-4 days before flood)\n",
        "- **144h**: Peak flooding 144-121 hours before (5-6 days before flood)\n",
        "\n",
        "### Expected Output\n",
        "- **12 weather data files** in `weather_first_3_features_{time_window}/` directory\n",
        "- **12 extracted 24-hour files** in `weather_24h_extracted_{time_window}/` directory\n",
        "- **Complete metadata** for each sample including coordinates and peak dates\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
