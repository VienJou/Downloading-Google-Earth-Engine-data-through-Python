{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flooding Event Dataset Generation \n",
    "\n",
    "This notebook generates a comprehensive JSON dataset from flooding event data sources with optimized code structure.\n",
    "\n",
    "## Data Sources:\n",
    "1. **Unified_Peak_Data_2016_2017_with_ID(1006).csv** - Gage data for 2016-2017\n",
    "2. **Unified_Peak_Data_2018_and_later_with_ID(1006).csv** - Gage data for 2018+\n",
    "3. **matched_records_1947_with_ID_2016_2017(1006).csv** - HWM data for 2016-2017\n",
    "4. **matched_records_698_with_ID_2018_and_later(1006).csv** - HWM data for 2018+\n",
    "\n",
    "## File Mapping:\n",
    "- **Numpy files (embedding)**: \n",
    "  - embedding_1Y_later/ - for 2016-2017 data\n",
    "  - embedding_1Y_early/ - for 2018+ data\n",
    "- **Weather files**: \n",
    "  - gage_2016_2017_24h/ and hwm_2016_2017_24h/ - for 2016-2017 data\n",
    "  - gage_2018_later_24h/ and hwm_2018_later_24h/ - for 2018+ data\n",
    "\n",
    "## Naming Convention:\n",
    "- **Gage data**: gage_[ID].npz, gage_[ID].csv\n",
    "- **HWM data**: HWM_[ID].npz, HWM_[ID].csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and File Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pathï¼š\n",
      "\n",
      "Input CSV files:\n",
      "  gage_2016_2017: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/Unified_Peak_Data_2016_2017_with_ID(1006).csv\n",
      "  gage_2018_later: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/Unified_Peak_Data_2018_and_later_with_ID(1006).csv\n",
      "  hwm_2016_2017: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/matched_records_1947_with_ID_2016_2017(1006).csv\n",
      "  hwm_2018_later: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/matched_records_698_with_ID_2018_and_later(1006).csv\n",
      "\n",
      "Numpy directories:\n",
      "  embedding_1Y_later: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later\n",
      "  embedding_1Y_early: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early\n",
      "\n",
      "Weather directories:\n",
      "  hwm_2018_later: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/weather_24h_extracted_24h/hwm_2018_later_96h_24hselected\n",
      "\n",
      "Output directory:\n",
      "  /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Define the input CSV file path\n",
    "csv_files = {\n",
    "    'gage_2016_2017': os.path.join(base_dir, 'csv_data', 'Unified_Peak_Data_2016_2017_with_ID(1006).csv'),\n",
    "    'gage_2018_later': os.path.join(base_dir, 'csv_data', 'Unified_Peak_Data_2018_and_later_with_ID(1006).csv'),\n",
    "    'hwm_2016_2017': os.path.join(base_dir, 'csv_data', 'matched_records_1947_with_ID_2016_2017(1006).csv'),\n",
    "    'hwm_2018_later': os.path.join(base_dir, 'csv_data', 'matched_records_698_with_ID_2018_and_later(1006).csv')\n",
    "}\n",
    "\n",
    "# Define the embedded data catalog (.npz file)\n",
    "numpy_dirs = {\n",
    "    'embedding_1Y_later': os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_later'),\n",
    "    'embedding_1Y_early': os.path.join(base_dir, 'Flood_dataset', 'embedding_1Y_early')\n",
    "}\n",
    "\n",
    "# Define the weather data directory (.csv files)\n",
    "weather_dirs = {\n",
    "    'hwm_2018_later': os.path.join(base_dir, 'csv_data', 'weather_24h_extracted_24h', 'hwm_2018_later_96h_24hselected')\n",
    "    # If you also need to add paths for gage or other time periods, you can expand them here.\n",
    "}\n",
    "\n",
    "# define output path\n",
    "output_dir = os.path.join(base_dir, 'Flood_dataset')\n",
    "\n",
    "# print to confirm\n",
    "print(\" pathï¼š\\n\")\n",
    "print(\"Input CSV files:\")\n",
    "for key, path in csv_files.items():\n",
    "    print(f\"  {key}: {path}\")\n",
    "\n",
    "print(\"\\nNumpy directories:\")\n",
    "for key, path in numpy_dirs.items():\n",
    "    print(f\"  {key}: {path}\")\n",
    "\n",
    "print(\"\\nWeather directories:\")\n",
    "for key, path in weather_dirs.items():\n",
    "    print(f\"  {key}: {path}\")\n",
    "\n",
    "print(f\"\\nOutput directory:\\n  {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined successfully!\n",
      "CORRECTED VERSION: Now properly extracts elevation_m and height_above_gnd_m from CSV files\n"
     ]
    }
   ],
   "source": [
    "def scan_directory_files(directory):\n",
    "    \"\"\"\n",
    "    Scan a directory and return a dictionary mapping file IDs to full paths.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to scan\n",
    "        \n",
    "    Returns:\n",
    "        dict: {file_id: full_path} mapping\n",
    "    \"\"\"\n",
    "    file_map = {}\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Warning: Directory not found: {directory}\")\n",
    "        return file_map\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.npz') or filename.endswith('.csv'):\n",
    "            # Extract ID from filename\n",
    "            # Format: gage_[ID].ext or HWM_[ID].ext\n",
    "            basename = filename.rsplit('.', 1)[0]  # Remove extension\n",
    "            if '_' in basename:\n",
    "                file_id = basename.split('_', 1)[1]  # Get part after first underscore\n",
    "                full_path = os.path.join(directory, filename)\n",
    "                file_map[file_id] = full_path\n",
    "    \n",
    "    return file_map\n",
    "\n",
    "def find_matching_files(record_id, data_type, period):\n",
    "    \"\"\"\n",
    "    Find matching numpy and weather files based on ID, data type, and period.\n",
    "    \n",
    "    Args:\n",
    "        record_id (str): Record ID to match\n",
    "        data_type (str): 'gage' or 'hwm'\n",
    "        period (str): '2016_2017' or '2018_later'\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (numpy_file_path, weather_file_path) with full paths or (None, None)\n",
    "    \"\"\"\n",
    "    numpy_file = None\n",
    "    weather_file = None\n",
    "    \n",
    "    # Determine numpy directory based on period\n",
    "    if period == '2016_2017':\n",
    "        numpy_dir = numpy_dirs['embedding_1Y_later']\n",
    "    else:  # 2018_later\n",
    "        numpy_dir = numpy_dirs['embedding_1Y_early']\n",
    "    \n",
    "    # Determine weather directory based on data type and period\n",
    "    weather_key = f\"{data_type}_{period}\"\n",
    "    weather_dir = weather_dirs.get(weather_key)\n",
    "    \n",
    "    if not weather_dir:\n",
    "        return None, None\n",
    "    \n",
    "    # Build expected filenames\n",
    "    if data_type == 'gage':\n",
    "        numpy_filename = f\"gage_{record_id}.npz\"\n",
    "        weather_filename = f\"gage_{record_id}.csv\"\n",
    "    else:  # hwm\n",
    "        numpy_filename = f\"HWM_{record_id}.npz\"\n",
    "        weather_filename = f\"HWM_{record_id}.csv\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    numpy_path = os.path.join(numpy_dir, numpy_filename)\n",
    "    weather_path = os.path.join(weather_dir, weather_filename)\n",
    "    \n",
    "    if os.path.exists(numpy_path):\n",
    "        numpy_file = numpy_path\n",
    "    \n",
    "    if os.path.exists(weather_path):\n",
    "        weather_file = weather_path\n",
    "    \n",
    "    return numpy_file, weather_file\n",
    "\n",
    "def safe_float(value):\n",
    "    \"\"\"\n",
    "    Safely convert value to float, return None if invalid.\n",
    "    \n",
    "    Args:\n",
    "        value: Value to convert (can be number, string, or None)\n",
    "        \n",
    "    Returns:\n",
    "        float: Converted value, or None if invalid\n",
    "    \"\"\"\n",
    "    if value is None or pd.isna(value) or str(value).strip() in ['', 'nan', 'NaN', 'None']:\n",
    "        return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def extract_record(row, data_type):\n",
    "    \"\"\"\n",
    "    Extract required fields from CSV row (CORRECTED VERSION).\n",
    "    \n",
    "    IMPORTANT CHANGES:\n",
    "    - For gage data: Uses elevation_m and height_above_gnd_m from CSV (already in meters)\n",
    "    - For hwm data: Uses elev_ft_m as elevation_m and height_above_gnd_m from CSV (already converted)\n",
    "    \n",
    "    Args:\n",
    "        row (dict): CSV row data\n",
    "        data_type (str): 'gage' or 'hwm'\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed record\n",
    "    \"\"\"\n",
    "    if data_type == 'gage':\n",
    "        # Gage data (Unified_Peak_Data)\n",
    "        # CSV files have been updated with height_above_gnd_m column (converted from ft to m)\n",
    "        return {\n",
    "            \"ID\": row.get(\"ID\", \"\"),\n",
    "            \"latitude\": safe_float(row.get(\"latitude\")),\n",
    "            \"longitude\": safe_float(row.get(\"longitude\")),\n",
    "            \"Ground_Elevation_m\": safe_float(row.get(\"Ground_Elevation_m\")),\n",
    "            \"site_no\": row.get(\"site_no\", \"\"),\n",
    "            \"station_id\": row.get(\"station_id\", \"\"),\n",
    "            \"peak_date\": row.get(\"peak_date\", \"\"),\n",
    "            \"elevation_m\": safe_float(row.get(\"elevation_m\")),\n",
    "            \"peak_stage\": safe_float(row.get(\"peak_stage\")),\n",
    "            \"event\": row.get(\"event\", \"\"),\n",
    "            \"source\": row.get(\"source\", \"\"),\n",
    "            \"height_above_gnd_m\": safe_float(row.get(\"height_above_gnd_m\")),  # Use converted column\n",
    "            \"data_type\": \"gage\"\n",
    "        }\n",
    "    else:\n",
    "        # HWM data (matched_records)\n",
    "        # CORRECTED: Use elev_ft_m as elevation_m, and height_above_gnd_m (both already converted to meters)\n",
    "        return {\n",
    "            \"ID\": row.get(\"ID\", \"\"),\n",
    "            \"latitude\": safe_float(row.get(\"latitude\")),\n",
    "            \"longitude\": safe_float(row.get(\"longitude\")),\n",
    "            \"site_no\": row.get(\"site_no\", \"\"),\n",
    "            \"elevation_m\": safe_float(row.get(\"elev_ft_m\")),  # CORRECTED: Use elev_ft_m\n",
    "            \"peak_stage\": safe_float(row.get(\"elev_ft_m\")),   # HWM uses same value for peak_stage\n",
    "            \"height_above_gnd_m\": safe_float(row.get(\"height_above_gnd_m\")),  # CORRECTED: Use converted column\n",
    "            \"data_type\": \"hwm\"\n",
    "        }\n",
    "\n",
    "print(\"Utility functions defined successfully!\")\n",
    "print(\"CORRECTED VERSION: Now properly extracts elevation_m and height_above_gnd_m from CSV files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Available Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directories and counting files...\n",
      "============================================================\n",
      "Numpy directory: embedding_1Y_later\n",
      "  Path: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_later\n",
      "  Files: 0\n",
      "Numpy directory: embedding_1Y_early\n",
      "  Path: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/embedding_1Y_early\n",
      "  Files: 0\n",
      "\n",
      "Weather directory: hwm_2018_later\n",
      "  Path: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/csv_data/weather_24h_extracted_24h/hwm_2018_later_96h_24hselected\n",
      "  Files: 3\n",
      "  Sample: 24h_1949.csv, 24h_1950.csv, 24h_1951.csv\n",
      "\n",
      "Directory scanning completed!\n"
     ]
    }
   ],
   "source": [
    "# Count files in each directory\n",
    "print(\"Scanning directories and counting files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scan numpy directories\n",
    "numpy_file_counts = {}\n",
    "for key, directory in numpy_dirs.items():\n",
    "    if os.path.exists(directory):\n",
    "        files = [f for f in os.listdir(directory) if f.endswith('.npz')]\n",
    "        numpy_file_counts[key] = len(files)\n",
    "        print(f\"Numpy directory: {key}\")\n",
    "        print(f\"  Path: {directory}\")\n",
    "        print(f\"  Files: {len(files)}\")\n",
    "        if files[:3]:\n",
    "            print(f\"  Sample: {', '.join(files[:3])}\")\n",
    "    else:\n",
    "        numpy_file_counts[key] = 0\n",
    "        print(f\"Numpy directory: {key} - NOT FOUND\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Scan weather directories\n",
    "weather_file_counts = {}\n",
    "for key, directory in weather_dirs.items():\n",
    "    if os.path.exists(directory):\n",
    "        files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "        weather_file_counts[key] = len(files)\n",
    "        print(f\"Weather directory: {key}\")\n",
    "        print(f\"  Path: {directory}\")\n",
    "        print(f\"  Files: {len(files)}\")\n",
    "        if files[:3]:\n",
    "            print(f\"  Sample: {', '.join(files[:3])}\")\n",
    "    else:\n",
    "        weather_file_counts[key] = 0\n",
    "        print(f\"Weather directory: {key} - NOT FOUND\")\n",
    "\n",
    "print()\n",
    "print(\"Directory scanning completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All CSV Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all CSV files...\n",
      "============================================================\n",
      "\n",
      "Processing: gage_2016_2017\n",
      "  Records: 3\n",
      "  Processed: 3 records\n",
      "\n",
      "Processing: gage_2018_later\n",
      "  Records: 3\n",
      "  Processed: 3 records\n",
      "\n",
      "Processing: hwm_2016_2017\n",
      "  Records: 3\n",
      "  Processed: 3 records\n",
      "\n",
      "Processing: hwm_2018_later\n",
      "  Records: 3\n",
      "  Processed: 3 records\n",
      "\n",
      "============================================================\n",
      "Processing completed: 12 total records\n",
      "  Gage: 6, HWM: 6\n",
      "  With numpy: 0 (0.0%)\n",
      "  With weather: 0 (0.0%)\n",
      "  With both files: 0 (0.0%)\n",
      "  With both elevations: 3 (25.0%)\n"
     ]
    }
   ],
   "source": [
    "def calculate_comprehensive_stats(all_data):\n",
    "    \"\"\"Calculate all statistics in one pass to avoid redundancy\"\"\"\n",
    "    stats = {\n",
    "        'total_records': len(all_data),\n",
    "        'gage_records': 0,\n",
    "        'hwm_records': 0,\n",
    "        'records_with_numpy': 0,\n",
    "        'records_with_weather': 0,\n",
    "        'records_with_both': 0,\n",
    "        'records_with_elevation': 0,\n",
    "        'records_with_height': 0,\n",
    "        'records_with_both_elev_height': 0,\n",
    "        'by_period': {'2016_2017': 0, '2018_later': 0},\n",
    "        'by_type_period': {}\n",
    "    }\n",
    "    \n",
    "    for record in all_data:\n",
    "        # Basic counts\n",
    "        if record['data_type'] == 'gage':\n",
    "            stats['gage_records'] += 1\n",
    "        else:\n",
    "            stats['hwm_records'] += 1\n",
    "        \n",
    "        # File availability\n",
    "        if record['numpy_file']:\n",
    "            stats['records_with_numpy'] += 1\n",
    "        if record['weather_file']:\n",
    "            stats['records_with_weather'] += 1\n",
    "        if record['numpy_file'] and record['weather_file']:\n",
    "            stats['records_with_both'] += 1\n",
    "        \n",
    "        # Elevation data\n",
    "        if record.get('elevation_m') is not None:\n",
    "            stats['records_with_elevation'] += 1\n",
    "        if record.get('height_above_gnd_m') is not None:\n",
    "            stats['records_with_height'] += 1\n",
    "        if record.get('elevation_m') is not None and record.get('height_above_gnd_m') is not None:\n",
    "            stats['records_with_both_elev_height'] += 1\n",
    "        \n",
    "        # Period counts\n",
    "        period = record.get('period', 'unknown')\n",
    "        if period in stats['by_period']:\n",
    "            stats['by_period'][period] += 1\n",
    "        \n",
    "        # Type-period combinations\n",
    "        key = f\"{record['data_type']}_{period}\"\n",
    "        stats['by_type_period'][key] = stats['by_type_period'].get(key, 0) + 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Processing all CSV files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Process each CSV file\n",
    "for file_key, csv_path in csv_files.items():\n",
    "    print(f\"\\nProcessing: {file_key}\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"  ERROR: File not found!\")\n",
    "        continue\n",
    "    \n",
    "    # Determine data type and period\n",
    "    data_type = 'gage' if 'gage' in file_key else 'hwm'\n",
    "    period = '2016_2017' if '2016_2017' in file_key else '2018_later'\n",
    "    \n",
    "    # Read and process CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(f\"  Records: {len(df)}\")\n",
    "    \n",
    "    # Process each record\n",
    "    for idx, row in df.iterrows():\n",
    "        record = extract_record(row.to_dict(), data_type)\n",
    "        record_id = str(row.get('ID', ''))\n",
    "        \n",
    "        if not record_id:\n",
    "            continue\n",
    "        \n",
    "        # Find matching files and add metadata\n",
    "        numpy_file, weather_file = find_matching_files(record_id, data_type, period)\n",
    "        record.update({\n",
    "            'numpy_file': numpy_file,\n",
    "            'weather_file': weather_file,\n",
    "            'period': period\n",
    "        })\n",
    "        \n",
    "        all_data.append(record)\n",
    "    \n",
    "    print(f\"  Processed: {len(df)} records\")\n",
    "\n",
    "# Calculate all statistics once\n",
    "stats = calculate_comprehensive_stats(all_data)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Processing completed: {stats['total_records']} total records\")\n",
    "print(f\"  Gage: {stats['gage_records']}, HWM: {stats['hwm_records']}\")\n",
    "print(f\"  With numpy: {stats['records_with_numpy']} ({stats['records_with_numpy']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  With weather: {stats['records_with_weather']} ({stats['records_with_weather']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  With both files: {stats['records_with_both']} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  With both elevations: {stats['records_with_both_elev_height']} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Statistics and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Statistics Summary\n",
      "============================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Total records: 12\n",
      "  Gage records: 6\n",
      "  HWM records: 6\n",
      "\n",
      "Detailed Statistics by Type-Period:\n",
      "  gage_2016_2017: 3 records\n",
      "  gage_2018_later: 3 records\n",
      "  hwm_2016_2017: 3 records\n",
      "  hwm_2018_later: 3 records\n",
      "\n",
      "File Matching Summary:\n",
      "  Records with numpy files: 0 (0.0%)\n",
      "  Records with weather files: 0 (0.0%)\n",
      "  Records with both files: 0 (0.0%)\n",
      "\n",
      "Elevation Data Summary:\n",
      "  Records with elevation_m: 12 (100.0%)\n",
      "  Records with height_above_gnd_m: 3 (25.0%)\n",
      "  Records with both elevations: 3 (25.0%)\n",
      "\n",
      "Data Completeness:\n",
      "  Records ready for analysis: 0/12\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Statistics Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"  Total records: {stats['total_records']}\")\n",
    "print(f\"  Gage records: {stats['gage_records']}\")\n",
    "print(f\"  HWM records: {stats['hwm_records']}\")\n",
    "\n",
    "print(f\"\\nDetailed Statistics by Type-Period:\")\n",
    "for key, count in stats['by_type_period'].items():\n",
    "    print(f\"  {key}: {count} records\")\n",
    "\n",
    "print(f\"\\nFile Matching Summary:\")\n",
    "print(f\"  Records with numpy files: {stats['records_with_numpy']} ({stats['records_with_numpy']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  Records with weather files: {stats['records_with_weather']} ({stats['records_with_weather']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  Records with both files: {stats['records_with_both']} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nElevation Data Summary:\")\n",
    "print(f\"  Records with elevation_m: {stats['records_with_elevation']} ({stats['records_with_elevation']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  Records with height_above_gnd_m: {stats['records_with_height']} ({stats['records_with_height']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  Records with both elevations: {stats['records_with_both_elev_height']} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nData Completeness:\")\n",
    "print(f\"  Records ready for analysis: {stats['records_with_both']}/{stats['total_records']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSON Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating JSON dataset...\n",
      "Output file: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/flooding_dataset_optimized_20251022_161021.json\n",
      "âœ“ JSON dataset generated successfully!\n",
      "  File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/flooding_dataset_optimized_20251022_161021.json\n",
      "  Size: 0.01 MB\n",
      "  Records: 12\n",
      "  Records with both files: 0 (0.0%)\n",
      "  Records with both elevations: 3 (25.0%)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_metadata(stats, numpy_file_counts, weather_file_counts):\n",
    "    \"\"\"Create comprehensive metadata using pre-calculated statistics\"\"\"\n",
    "    return {\n",
    "        \"description\": \"Comprehensive flooding event dataset with file paths (Optimized Version)\",\n",
    "        \"version\": \"5.0\",\n",
    "        \"generated_timestamp\": datetime.now().isoformat(),\n",
    "        \"total_records\": stats['total_records'],\n",
    "        \"gage_records\": stats['gage_records'],\n",
    "        \"hwm_records\": stats['hwm_records'],\n",
    "        \"data_sources\": csv_files,\n",
    "        \"file_directories\": {\n",
    "            \"numpy\": numpy_dirs,\n",
    "            \"weather\": weather_dirs\n",
    "        },\n",
    "        \"file_counts\": {\n",
    "            \"numpy_files\": numpy_file_counts,\n",
    "            \"weather_files\": weather_file_counts\n",
    "        },\n",
    "        \"statistics\": {\n",
    "            \"records_with_numpy\": stats['records_with_numpy'],\n",
    "            \"records_with_weather\": stats['records_with_weather'],\n",
    "            \"records_with_both\": stats['records_with_both'],\n",
    "            \"records_with_elevation_m\": stats['records_with_elevation'],\n",
    "            \"records_with_height_above_gnd_m\": stats['records_with_height'],\n",
    "            \"records_with_both_elev_height\": stats['records_with_both_elev_height'],\n",
    "            \"matching_rates\": {\n",
    "                \"numpy\": f\"{stats['records_with_numpy']/stats['total_records']*100:.2f}%\",\n",
    "                \"weather\": f\"{stats['records_with_weather']/stats['total_records']*100:.2f}%\",\n",
    "                \"both\": f\"{stats['records_with_both']/stats['total_records']*100:.2f}%\",\n",
    "                \"both_elev_height\": f\"{stats['records_with_both_elev_height']/stats['total_records']*100:.2f}%\"\n",
    "            }\n",
    "        },\n",
    "        \"by_type_period\": stats['by_type_period'],\n",
    "        \"notes\": [\n",
    "            \"numpy_file and weather_file fields contain full absolute paths\",\n",
    "            \"Gage data uses prefix 'gage_' for files\",\n",
    "            \"HWM data uses prefix 'HWM_' for files\",\n",
    "            \"2016-2017 data uses embedding_1Y_later directory\",\n",
    "            \"2018+ data uses embedding_1Y_early directory\",\n",
    "            \"All elevation values are in meters\",\n",
    "            \"Optimized version with consolidated statistics\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Create output filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = os.path.join(output_dir, f\"flooding_dataset_optimized_{timestamp}.json\")\n",
    "\n",
    "print(f\"Generating JSON dataset...\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "\n",
    "# Create dataset structure using pre-calculated statistics\n",
    "dataset = {\n",
    "    \"metadata\": create_dataset_metadata(stats, numpy_file_counts, weather_file_counts),\n",
    "    \"data\": all_data\n",
    "}\n",
    "\n",
    "# Write JSON file\n",
    "try:\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
    "    print(f\"âœ“ JSON dataset generated successfully!\")\n",
    "    print(f\"  File: {output_file}\")\n",
    "    print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Records: {stats['total_records']:,}\")\n",
    "    print(f\"  Records with both files: {stats['records_with_both']:,} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
    "    print(f\"  Records with both elevations: {stats['records_with_both_elev_height']:,} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Record Verification\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Dataset Generation Summary\n",
      "============================================================\n",
      "âœ“ Total records processed: 12\n",
      "âœ“ Gage records: 6\n",
      "âœ“ HWM records: 6\n",
      "âœ“ Records with both files: 0 (0.0%)\n",
      "âœ“ Records with both elevations: 3 (25.0%)\n",
      "\n",
      "âœ“ JSON file generated: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/flooding_dataset_optimized_20251022_161021.json\n",
      "âœ“ File size: 0.01 MB\n",
      "\n",
      "Dataset is ready for analysis! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "def verify_sample_records(all_data):\n",
    "    \"\"\"Verify sample records and check file existence\"\"\"\n",
    "    print(\"Sample Record Verification\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check a gage record\n",
    "    gage_samples = [r for r in all_data if r['data_type'] == 'gage' and r['numpy_file']]\n",
    "    if gage_samples:\n",
    "        sample = gage_samples[0]\n",
    "        print(f\"\\n1. Sample Gage Record:\")\n",
    "        print(f\"  ID: {sample['ID']}, Site: {sample['site_no']}\")\n",
    "        print(f\"  Coordinates: ({sample['latitude']}, {sample['longitude']})\")\n",
    "        print(f\"  Files exist: numpy={os.path.exists(sample['numpy_file'])}, weather={os.path.exists(sample['weather_file']) if sample['weather_file'] else False}\")\n",
    "    \n",
    "    # Check a HWM record\n",
    "    hwm_samples = [r for r in all_data if r['data_type'] == 'hwm' and r['numpy_file']]\n",
    "    if hwm_samples:\n",
    "        sample = hwm_samples[0]\n",
    "        print(f\"\\n2. Sample HWM Record:\")\n",
    "        print(f\"  ID: {sample['ID']}, Site: {sample['site_no']}\")\n",
    "        print(f\"  Coordinates: ({sample['latitude']}, {sample['longitude']})\")\n",
    "        print(f\"  Files exist: numpy={os.path.exists(sample['numpy_file'])}, weather={os.path.exists(sample['weather_file']) if sample['weather_file'] else False}\")\n",
    "\n",
    "# Verify sample records\n",
    "verify_sample_records(all_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Dataset Generation Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ“ Total records processed: {stats['total_records']:,}\")\n",
    "print(f\"âœ“ Gage records: {stats['gage_records']:,}\")\n",
    "print(f\"âœ“ HWM records: {stats['hwm_records']:,}\")\n",
    "print(f\"âœ“ Records with both files: {stats['records_with_both']:,} ({stats['records_with_both']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"âœ“ Records with both elevations: {stats['records_with_both_elev_height']:,} ({stats['records_with_both_elev_height']/stats['total_records']*100:.1f}%)\")\n",
    "print(f\"\\nâœ“ JSON file generated: {output_file}\")\n",
    "print(f\"âœ“ File size: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(\"\\nDataset is ready for analysis! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Filtered Version (Optional)\n",
    "\n",
    "Generate a filtered version containing only records with both elevation_m and height_above_gnd_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating filtered version...\n",
      "============================================================\n",
      "Filtered records: 3 / 12\n",
      "  Gage: 0, HWM: 3\n",
      "\n",
      "============================================================\n",
      "âœ“ Filtered version generated!\n",
      "  File: /home/jovyan/Downloading-Google-Earth-Engine-data-through-Python/flooding_event_processing/Flood_dataset/flooding_dataset_filtered_with_both_elevations_3items.json\n",
      "  Size: 0.00 MB\n",
      "  Records: 3\n",
      "  Percentage: 25.0% of total\n"
     ]
    }
   ],
   "source": [
    "def generate_filtered_dataset(all_data, stats, output_dir, source_file):\n",
    "    \"\"\"Generate filtered dataset with both elevation values\"\"\"\n",
    "    print(\"Generating filtered version...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter records using pre-calculated stats\n",
    "    filtered_records = [r for r in all_data \n",
    "                        if r.get('elevation_m') is not None \n",
    "                        and r.get('height_above_gnd_m') is not None]\n",
    "    \n",
    "    # Calculate filtered statistics\n",
    "    gage_filtered = len([r for r in filtered_records if r['data_type'] == 'gage'])\n",
    "    hwm_filtered = len([r for r in filtered_records if r['data_type'] == 'hwm'])\n",
    "    \n",
    "    print(f\"Filtered records: {len(filtered_records)} / {stats['total_records']}\")\n",
    "    print(f\"  Gage: {gage_filtered}, HWM: {hwm_filtered}\")\n",
    "    \n",
    "    # Create filtered dataset\n",
    "    filtered_dataset = {\n",
    "        \"metadata\": {\n",
    "            \"description\": \"Filtered flooding event dataset with both elevation_m and height_above_gnd_m values (Optimized Version)\",\n",
    "            \"version\": \"5.0_filtered\",\n",
    "            \"generated_timestamp\": datetime.now().isoformat(),\n",
    "            \"source_file\": source_file,\n",
    "            \"filter_criteria\": \"Records with both elevation_m and height_above_gnd_m not null\",\n",
    "            \"original_total_records\": stats['total_records'],\n",
    "            \"filtered_total_records\": len(filtered_records),\n",
    "            \"total_records\": len(filtered_records),\n",
    "            \"gage_records\": gage_filtered,\n",
    "            \"hwm_records\": hwm_filtered,\n",
    "            \"notes\": [\n",
    "                f\"Filtered from {stats['total_records']} records to {len(filtered_records)} records\",\n",
    "                \"All records have both elevation_m and height_above_gnd_m values\",\n",
    "                \"Optimized version with consolidated statistics\"\n",
    "            ]\n",
    "        },\n",
    "        \"data\": filtered_records\n",
    "    }\n",
    "    \n",
    "    # Save filtered file\n",
    "    filtered_output = os.path.join(output_dir, f\"flooding_dataset_filtered_with_both_elevations_{len(filtered_records)}items.json\")\n",
    "    \n",
    "    with open(filtered_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(filtered_output) / 1024 / 1024\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"âœ“ Filtered version generated!\")\n",
    "    print(f\"  File: {filtered_output}\")\n",
    "    print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Records: {len(filtered_records):,}\")\n",
    "    print(f\"  Percentage: {len(filtered_records)/stats['total_records']*100:.1f}% of total\")\n",
    "    \n",
    "    return filtered_output\n",
    "\n",
    "# Generate filtered version\n",
    "filtered_output = generate_filtered_dataset(all_data, stats, output_dir, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Use the Generated Dataset\n",
      "============================================================\n",
      "\n",
      "# Load the JSON dataset\n",
      "import json\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Load JSON file\n",
      "with open('flooding_dataset_updated_TIMESTAMP.json', 'r') as f:\n",
      "    dataset = json.load(f)\n",
      "\n",
      "# Access metadata\n",
      "metadata = dataset['metadata']\n",
      "print(f\"Total records: {metadata['total_records']}\")\n",
      "print(f\"Gage records: {metadata['gage_records']}\")\n",
      "print(f\"HWM records: {metadata['hwm_records']}\")\n",
      "\n",
      "# Access data records\n",
      "data_records = dataset['data']\n",
      "\n",
      "# Filter records with both files\n",
      "complete_records = [r for r in data_records if r['numpy_file'] and r['weather_file']]\n",
      "print(f\"Complete records: {len(complete_records)}\")\n",
      "\n",
      "# Example: Load a specific record's data\n",
      "record = complete_records[0]\n",
      "print(f\"\\nRecord ID: {record['ID']}\")\n",
      "print(f\"Type: {record['data_type']}\")\n",
      "print(f\"Location: ({record['latitude']}, {record['longitude']})\")\n",
      "\n",
      "# Load numpy embedding\n",
      "embedding = np.load(record['numpy_file'])\n",
      "print(f\"Embedding shape: {embedding['image_data'].shape}\")\n",
      "\n",
      "# Load weather data\n",
      "weather_df = pd.read_csv(record['weather_file'])\n",
      "print(f\"Weather data shape: {weather_df.shape}\")\n",
      "print(f\"Weather columns: {list(weather_df.columns)}\")\n",
      "\n",
      "# Example: Batch processing\n",
      "for record in complete_records[:10]:  # Process first 10 records\n",
      "    # Load embedding\n",
      "    embedding = np.load(record['numpy_file'])\n",
      "    \n",
      "    # Load weather\n",
      "    weather = pd.read_csv(record['weather_file'])\n",
      "    \n",
      "    # Your processing logic here\n",
      "    pass\n",
      "\n",
      "\n",
      "Key Features:\n",
      "  â€¢ All file paths are absolute paths\n",
      "  â€¢ Easy to filter by data_type (gage/hwm) and period (2016_2017/2018_later)\n",
      "  â€¢ Metadata provides complete statistics and directory information\n",
      "  â€¢ Ready for machine learning and analysis workflows\n",
      "\n",
      "============================================================\n",
      "Dataset generation completed successfully! ðŸŽ‰\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: How to load and use the generated JSON dataset\n",
    "print(\"How to Use the Generated Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "# Load the JSON dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load JSON file\n",
    "with open('flooding_dataset_updated_TIMESTAMP.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Access metadata\n",
    "metadata = dataset['metadata']\n",
    "print(f\"Total records: {metadata['total_records']}\")\n",
    "print(f\"Gage records: {metadata['gage_records']}\")\n",
    "print(f\"HWM records: {metadata['hwm_records']}\")\n",
    "\n",
    "# Access data records\n",
    "data_records = dataset['data']\n",
    "\n",
    "# Filter records with both files\n",
    "complete_records = [r for r in data_records if r['numpy_file'] and r['weather_file']]\n",
    "print(f\"Complete records: {len(complete_records)}\")\n",
    "\n",
    "# Example: Load a specific record's data\n",
    "record = complete_records[0]\n",
    "print(f\"\\\\nRecord ID: {record['ID']}\")\n",
    "print(f\"Type: {record['data_type']}\")\n",
    "print(f\"Location: ({record['latitude']}, {record['longitude']})\")\n",
    "\n",
    "# Load numpy embedding\n",
    "embedding = np.load(record['numpy_file'])\n",
    "print(f\"Embedding shape: {embedding['image_data'].shape}\")\n",
    "\n",
    "# Load weather data\n",
    "weather_df = pd.read_csv(record['weather_file'])\n",
    "print(f\"Weather data shape: {weather_df.shape}\")\n",
    "print(f\"Weather columns: {list(weather_df.columns)}\")\n",
    "\n",
    "# Example: Batch processing\n",
    "for record in complete_records[:10]:  # Process first 10 records\n",
    "    # Load embedding\n",
    "    embedding = np.load(record['numpy_file'])\n",
    "    \n",
    "    # Load weather\n",
    "    weather = pd.read_csv(record['weather_file'])\n",
    "    \n",
    "    # Your processing logic here\n",
    "    pass\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  â€¢ All file paths are absolute paths\")\n",
    "print(\"  â€¢ Easy to filter by data_type (gage/hwm) and period (2016_2017/2018_later)\")\n",
    "print(\"  â€¢ Metadata provides complete statistics and directory information\")\n",
    "print(\"  â€¢ Ready for machine learning and analysis workflows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Dataset generation completed successfully! ðŸŽ‰\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoai-Python3-0.9.5",
   "language": "python",
   "name": "geoai-py3-0.9.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
