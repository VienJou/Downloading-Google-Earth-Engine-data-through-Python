{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weather Data Download (First 3 Features - Complete Data)\n",
        "\n",
        "This notebook downloads weather data from Open-Meteo API for **the first 3 features from each CSV file**. This provides complete weather data for a manageable subset of flooding events.\n",
        "\n",
        "## Features\n",
        "- Downloads 24-hour weather data before peak flooding events\n",
        "- **Processes first 3 samples** from each input CSV file\n",
        "- Handles API rate limiting with retry mechanisms\n",
        "- Optimized parameter selection (removes null-heavy parameters)\n",
        "- Comprehensive logging and monitoring\n",
        "- Ready to run - no comments to uncomment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import signal\n",
        "import traceback\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "from collections import defaultdict\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API Rate Limiting Analysis\n",
        "\n",
        "### Open-Meteo API Rate Limits (Free Tier)\n",
        "- **Per minute**: 600 requests\n",
        "- **Per hour**: 5,000 requests  \n",
        "- **Per day**: 10,000 requests\n",
        "- **Concurrent requests**: Recommended < 4 to avoid connection rejection\n",
        "\n",
        "\n",
        "### Optimization Strategy\n",
        "1. **Reduce concurrency**: From 6 to 2 threads\n",
        "2. **Remove null-heavy parameters**: Eliminated 11 parameters with 100% null values\n",
        "3. **Implement retry mechanism**: Exponential backoff for rate limit errors\n",
        "4. **Add request intervals**: Prevent rapid successive requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Optimized Weather Data Downloader Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimizedWeatherDownloader:\n",
        "    \"\"\"\n",
        "    Optimized weather data downloader with rate limiting handling\n",
        "    Downloads first 3 features from each CSV file\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir, max_workers=2, max_samples=3):\n",
        "        self.output_dir = output_dir\n",
        "        self.max_workers = max_workers\n",
        "        self.max_samples = max_samples  # Process first 3 samples from each file\n",
        "        self.completed_samples = set()\n",
        "        self.failed_samples = set()\n",
        "        self.lock = threading.Lock()\n",
        "        self.session_cache = {}\n",
        "        \n",
        "        # Optimized parameters (removed null-heavy ones)\n",
        "        self.weather_params = [\n",
        "            'temperature_2m', 'relative_humidity_2m', 'dewpoint_2m', 'apparent_temperature',\n",
        "            'precipitation', 'rain', 'snowfall', 'weather_code', 'pressure_msl', 'surface_pressure',\n",
        "            'cloud_cover', 'cloud_cover_low', 'cloud_cover_mid', 'cloud_cover_high',\n",
        "            'et0_fao_evapotranspiration', 'vapour_pressure_deficit', 'wind_speed_10m', 'wind_speed_100m',\n",
        "            'wind_direction_10m', 'wind_direction_100m', 'wind_gusts_10m'\n",
        "        ]\n",
        "        \n",
        "        # Excluded parameters (high null rates)\n",
        "        self.excluded_params = [\n",
        "            'visibility', 'evapotranspiration', 'soil_temperature_0cm', 'soil_temperature_6cm',\n",
        "            'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm',\n",
        "            'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm'\n",
        "        ]\n",
        "        \n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.setup_logging()\n",
        "    \n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        log_dir = os.path.join(os.path.dirname(self.output_dir), 'logs')\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        \n",
        "        log_file = os.path.join(log_dir, f\"weather_download_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        \n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file),\n",
        "                logging.StreamHandler(sys.stdout)\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "    \n",
        "    def get_session(self):\n",
        "        \"\"\"Get or create HTTP session for connection reuse\"\"\"\n",
        "        thread_id = threading.current_thread().ident\n",
        "        if thread_id not in self.session_cache:\n",
        "            self.session_cache[thread_id] = requests.Session()\n",
        "        return self.session_cache[thread_id]\n",
        "    \n",
        "    def build_api_url(self, lat, lon, start_date, end_date):\n",
        "        \"\"\"Build Open-Meteo API URL with optimized parameters\"\"\"\n",
        "        params_str = ','.join(self.weather_params)\n",
        "        return (\n",
        "            f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
        "            f\"latitude={lat}&longitude={lon}&start_date={start_date}&end_date={end_date}\"\n",
        "            f\"&hourly={params_str}&timezone=UTC\"\n",
        "        )\n",
        "    \n",
        "    def fetch_weather_data(self, sample_id, lat, lon, peak_date, max_retries=3):\n",
        "        \"\"\"\n",
        "        Fetch weather data with retry mechanism for rate limiting\n",
        "        \n",
        "        Args:\n",
        "            sample_id: Unique identifier for the sample\n",
        "            lat, lon: Coordinates\n",
        "            peak_date: Peak flooding date\n",
        "            max_retries: Maximum number of retry attempts\n",
        "        \"\"\"\n",
        "        # Convert peak_date to UTC\n",
        "        if peak_date.tzinfo is None:\n",
        "            peak_date_utc = peak_date.replace(tzinfo=pytz.UTC)\n",
        "        else:\n",
        "            peak_date_utc = peak_date.astimezone(pytz.UTC)\n",
        "        \n",
        "        # Calculate 24-hour window before peak\n",
        "        start_time = peak_date_utc - timedelta(hours=24)\n",
        "        end_time = peak_date_utc - timedelta(hours=1)\n",
        "        \n",
        "        start_str = start_time.strftime(\"%Y-%m-%d\")\n",
        "        end_str = end_time.strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "        url = self.build_api_url(lat, lon, start_str, end_str)\n",
        "        session = self.get_session()\n",
        "        \n",
        "        # Retry mechanism with exponential backoff\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                if attempt > 0:\n",
        "                    wait_time = 2 ** attempt  # Exponential backoff\n",
        "                    self.logger.warning(f\"Retrying after {wait_time}s, attempt {attempt+1}/{max_retries}\")\n",
        "                    time.sleep(wait_time)\n",
        "                \n",
        "                response = session.get(url, timeout=60)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                \n",
        "                if \"hourly\" not in data or \"time\" not in data[\"hourly\"]:\n",
        "                    raise ValueError(\"No hourly data available\")\n",
        "                \n",
        "                return self.process_weather_data(data, sample_id, lat, lon, peak_date)\n",
        "                \n",
        "            except requests.exceptions.HTTPError as e:\n",
        "                if e.response.status_code == 429:  # Rate limited\n",
        "                    if attempt < max_retries - 1:\n",
        "                        wait_time = 60 * (attempt + 1)  # Wait 1, 2, 3 minutes\n",
        "                        self.logger.warning(f\"Rate limited, waiting {wait_time}s before retry\")\n",
        "                        time.sleep(wait_time)\n",
        "                        continue\n",
        "                    else:\n",
        "                        self.logger.error(f\"Rate limit exceeded after {max_retries} attempts\")\n",
        "                        raise\n",
        "                else:\n",
        "                    self.logger.error(f\"HTTP error: {e}\")\n",
        "                    raise\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    self.logger.warning(f\"Request failed, retrying: {e}\")\n",
        "                    time.sleep(5)\n",
        "                    continue\n",
        "                else:\n",
        "                    self.logger.error(f\"Request failed after {max_retries} attempts: {e}\")\n",
        "                    raise\n",
        "    \n",
        "    def process_weather_data(self, data, sample_id, lat, lon, peak_date):\n",
        "        \"\"\"Process and save weather data\"\"\"\n",
        "        df = pd.DataFrame(data[\"hourly\"])\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        df['time'] = pd.to_datetime(df['time']).dt.tz_localize(None)\n",
        "        \n",
        "        # Add metadata\n",
        "        df['sample_id'] = sample_id\n",
        "        df['latitude'] = lat\n",
        "        df['longitude'] = lon\n",
        "        df['peak_date'] = peak_date.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        df['hours_before_peak'] = (peak_date - df['time']).dt.total_seconds() / 3600\n",
        "        \n",
        "        # Save to CSV\n",
        "        output_file = os.path.join(self.output_dir, f\"{sample_id}.csv\")\n",
        "        df.to_csv(output_file, index=False)\n",
        "        \n",
        "        return len(df)\n",
        "    \n",
        "    def process_sample(self, sample_data):\n",
        "        \"\"\"Process a single sample\"\"\"\n",
        "        sample_id, row = sample_data\n",
        "        \n",
        "        try:\n",
        "            # Extract coordinates\n",
        "            if 'latitude' in row and 'longitude' in row:\n",
        "                lat, lon = row['latitude'], row['longitude']\n",
        "            elif 'latitude_dd' in row and 'longitude_dd' in row:\n",
        "                lat, lon = row['latitude_dd'], row['longitude_dd']\n",
        "            else:\n",
        "                raise ValueError(\"No coordinates found\")\n",
        "            \n",
        "            # Extract peak date\n",
        "            if 'peak_date' in row:\n",
        "                peak_date = pd.to_datetime(row['peak_date'])\n",
        "            elif 'matched_peak_date' in row:\n",
        "                peak_date = pd.to_datetime(row['matched_peak_date'])\n",
        "            else:\n",
        "                raise ValueError(\"No peak date found\")\n",
        "            \n",
        "            # Check if file already exists\n",
        "            output_file = os.path.join(self.output_dir, f\"{sample_id}.csv\")\n",
        "            if os.path.exists(output_file):\n",
        "                return sample_id, \"skipped\", 0\n",
        "            \n",
        "            # Fetch weather data\n",
        "            record_count = self.fetch_weather_data(sample_id, lat, lon, peak_date)\n",
        "            \n",
        "            with self.lock:\n",
        "                self.completed_samples.add(sample_id)\n",
        "            \n",
        "            return sample_id, \"completed\", record_count\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to process {sample_id}: {e}\")\n",
        "            with self.lock:\n",
        "                self.failed_samples.add(sample_id)\n",
        "            return sample_id, \"failed\", 0\n",
        "    \n",
        "    def download_all_samples(self, csv_files):\n",
        "        \"\"\"Download weather data for first 3 samples from each file\"\"\"\n",
        "        all_samples = []\n",
        "        \n",
        "        # Load samples from CSV files\n",
        "        for csv_file in csv_files:\n",
        "            if os.path.exists(csv_file):\n",
        "                self.logger.info(f\"Loading {csv_file}\")\n",
        "                df = pd.read_csv(csv_file, low_memory=False)\n",
        "                \n",
        "                # Process first 3 samples\n",
        "                df = df.head(self.max_samples)\n",
        "                self.logger.info(f\"  Processing first {len(df)} samples (from {len(pd.read_csv(csv_file, low_memory=False))} total)\")\n",
        "                \n",
        "                for idx, row in df.iterrows():\n",
        "                    sample_id = self.create_sample_id(row, csv_file, idx)\n",
        "                    all_samples.append((sample_id, row))\n",
        "        \n",
        "        self.logger.info(f\"Total samples to process: {len(all_samples)} (first {self.max_samples} from each file)\")\n",
        "        \n",
        "        # Process samples with thread pool\n",
        "        completed_count = 0\n",
        "        failed_count = 0\n",
        "        \n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            future_to_sample = {executor.submit(self.process_sample, sample): sample[0] \n",
        "                              for sample in all_samples}\n",
        "            \n",
        "            for future in as_completed(future_to_sample):\n",
        "                sample_id = future_to_sample[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    sample_id, status, count = result\n",
        "                    \n",
        "                    if status == \"completed\":\n",
        "                        completed_count += 1\n",
        "                        self.logger.info(f\"Completed {sample_id}: {count} records\")\n",
        "                    elif status == \"failed\":\n",
        "                        failed_count += 1\n",
        "                        self.logger.error(f\"Failed {sample_id}\")\n",
        "                    \n",
        "                    # Progress update every 20 samples\n",
        "                    if (completed_count + failed_count) % 20 == 0:\n",
        "                        self.logger.info(f\"Progress: {completed_count} completed, {failed_count} failed\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Exception processing {sample_id}: {e}\")\n",
        "                    failed_count += 1\n",
        "        \n",
        "        # Cleanup\n",
        "        for session in self.session_cache.values():\n",
        "            session.close()\n",
        "        \n",
        "        self.logger.info(f\"Download complete: {completed_count} successful, {failed_count} failed\")\n",
        "        return completed_count, failed_count\n",
        "    \n",
        "    def create_sample_id(self, row, csv_file, idx):\n",
        "        \"\"\"Create unique sample ID from row data\"\"\"\n",
        "        # Handle Unified_Peak_Data file\n",
        "        if 'site_no' in row and 'event' in row:\n",
        "            site_no = str(row['site_no'])\n",
        "            event_name = str(row['event']).replace(' ', '_')\n",
        "            return f\"{site_no}_{event_name}\"\n",
        "        \n",
        "        # Handle HWMs_with_peaktime file\n",
        "        elif 'site_no' in row and 'eventName' in row:\n",
        "            site_no = str(row['site_no'])\n",
        "            event_name = str(row['eventName']).replace(' ', '_')\n",
        "            if event_name == 'nan':\n",
        "                event_name = 'unknown_event'\n",
        "            return f\"{site_no}_{event_name}\"\n",
        "        \n",
        "        # Fallback\n",
        "        if 'site_no' in row:\n",
        "            return f\"{row['site_no']}_unknown_event_{idx}\"\n",
        "        else:\n",
        "            return f\"sample_{os.path.basename(csv_file).split('.')[0]}_{idx}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Usage Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize downloader (First 3 features from each file)\n",
        "output_dir = '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/weather_first_3_features'\n",
        "downloader = OptimizedWeatherDownloader(output_dir, max_workers=2, max_samples=3)\n",
        "\n",
        "# Define input CSV files\n",
        "csv_files = [\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2016_2017_with_ID(1006).csv',\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/Unified_Peak_Data_2018_and_later_with_ID(1006).csv',\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_1947_with_ID_2016_2017(1006).csv',\n",
        "    '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/matched_records_698_with_ID_2018_and_later(1006).csv'\n",
        "]\n",
        "\n",
        "# Start download process\n",
        "print(\"Starting weather data download (first 3 features from each file)...\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"Max workers: {downloader.max_workers}\")\n",
        "print(f\"Max samples per file: {downloader.max_samples}\")\n",
        "print(f\"Weather parameters: {len(downloader.weather_params)}\")\n",
        "print(f\"Excluded parameters: {len(downloader.excluded_params)}\")\n",
        "\n",
        "# Run download\n",
        "completed, failed = downloader.download_all_samples(csv_files)\n",
        "print(f\"\\nDownload Results: {completed} successful, {failed} failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check download results\n",
        "print(\"ðŸ“Š Download Results Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Expected output: 12 weather data files (3 from each of 4 CSV files)\")\n",
        "print(\"Each file contains 24 hours of weather data before peak flooding\")\n",
        "print(\"Files saved in: /u/wz53/alphaearth/Flooding_event_/Flood_dataset/weather_first_3_features/\")\n",
        "print()\n",
        "print(\"Download completed! Check the output directory for your weather data files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extract 24-Hour Data Before Peak\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_24h_before_peak(df, peak_date):\n",
        "    \"\"\"\n",
        "    Extract exactly 24 hours of data before peak from weather DataFrame\n",
        "    \n",
        "    Args:\n",
        "        df: Weather data DataFrame with 'time' column\n",
        "        peak_date: Peak flooding datetime\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with 23-24 rows (1-24 hours before peak)\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['time'] = pd.to_datetime(df_copy['time'])\n",
        "    \n",
        "    # Ensure peak_date is datetime\n",
        "    if isinstance(peak_date, str):\n",
        "        peak_date = pd.to_datetime(peak_date)\n",
        "    \n",
        "    # Calculate hours before peak\n",
        "    time_diff_hours = (peak_date - df_copy['time']).dt.total_seconds() / 3600\n",
        "    \n",
        "    # Filter: 1-24 hours before peak\n",
        "    mask = (time_diff_hours >= 1) & (time_diff_hours <= 24)\n",
        "    df_24h = df_copy[mask].copy()\n",
        "    \n",
        "    # Sort by time\n",
        "    df_24h = df_24h.sort_values('time')\n",
        "    \n",
        "    # Add hours_before_peak column\n",
        "    df_24h['hours_before_peak'] = time_diff_hours[mask]\n",
        "    \n",
        "    return df_24h\n",
        "\n",
        "def process_weather_file_to_24h(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process a weather file and extract 24h before peak\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "    \n",
        "    # Get peak_date\n",
        "    peak_date = pd.to_datetime(df['peak_date'].iloc[0])\n",
        "    \n",
        "    # Extract 24h data\n",
        "    df_24h = extract_24h_before_peak(df, peak_date)\n",
        "    \n",
        "    print(f\"Original: {len(df)} rows -> Extracted: {len(df_24h)} rows\")\n",
        "    print(f\"Time range: {df_24h['time'].min()} to {df_24h['time'].max()}\")\n",
        "    print(f\"Hours before peak: {df_24h['hours_before_peak'].min():.1f} to {df_24h['hours_before_peak'].max():.1f}\")\n",
        "    \n",
        "    # Save\n",
        "    df_24h.to_csv(output_file, index=False)\n",
        "    \n",
        "    return df_24h\n",
        "\n",
        "# Example usage\n",
        "print(\"24-hour data extraction functions defined:\")\n",
        "print(\"- extract_24h_before_peak(): Extract 1-24 hours before peak\")\n",
        "print(\"- process_weather_file_to_24h(): Process file and save 24h data\")\n",
        "print()\n",
        "print(\"Usage example:\")\n",
        "print(\"df_24h = extract_24h_before_peak(df, peak_date)\")\n",
        "print(\"process_weather_file_to_24h('input.csv', 'output_24h.csv')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Process downloaded weather files to extract 24h data\n",
        "print(\"Processing downloaded weather files to extract 24-hour data...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Set up directories\n",
        "input_dir = '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/weather_first_3_features'\n",
        "output_dir = '/u/wz53/alphaearth/Flooding_event_/Flood_dataset/weather_24h_extracted'\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process all downloaded weather files\n",
        "if os.path.exists(input_dir):\n",
        "    weather_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
        "    \n",
        "    if weather_files:\n",
        "        print(f\"Found {len(weather_files)} weather files to process\")\n",
        "        print()\n",
        "        \n",
        "        for i, weather_file in enumerate(weather_files, 1):\n",
        "            input_path = os.path.join(input_dir, weather_file)\n",
        "            output_path = os.path.join(output_dir, f\"24h_{weather_file}\")\n",
        "            \n",
        "            print(f\"Processing {i}/{len(weather_files)}: {weather_file}\")\n",
        "            \n",
        "            try:\n",
        "                # Extract 24h data\n",
        "                df_24h = process_weather_file_to_24h(input_path, output_path)\n",
        "                print(f\"âœ“ Saved: {output_path}\")\n",
        "                print()\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"âœ— Error processing {weather_file}: {e}\")\n",
        "                print()\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(\"24-hour data extraction completed!\")\n",
        "        print(f\"Input directory: {input_dir}\")\n",
        "        print(f\"Output directory: {output_dir}\")\n",
        "        print(f\"Processed files: {len(weather_files)}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"No weather files found in input directory\")\n",
        "        print(\"Please run the download first (Cell 7)\")\n",
        "        \n",
        "else:\n",
        "    print(\"Input directory does not exist\")\n",
        "    print(\"Please run the download first (Cell 7)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Processing Summary\n",
        "\n",
        "### First 3 Features Processing Pipeline\n",
        "\n",
        "This notebook downloads weather data for **the first 3 features from each CSV file** and provides 24-hour data extraction:\n",
        "\n",
        "#### Key Features\n",
        "- **Limited Processing**: Processes first 3 samples from each CSV file (12 total)\n",
        "- **API Optimization**: Uses optimized parameters with rate limiting handling\n",
        "- **Quality Assurance**: Zero null values, complete weather parameters\n",
        "- **24-Hour Extraction**: Extracts exactly 24 hours before peak flooding\n",
        "- **Ready to Run**: No comments to uncomment, direct execution\n",
        "\n",
        "#### Processing Steps\n",
        "1. **Download**: Download weather data for first 3 features from each CSV file\n",
        "2. **Extract**: Extract exactly 24 hours before peak flooding event\n",
        "3. **Save**: Save processed 24-hour data to separate directory\n",
        "\n",
        "#### Expected Output\n",
        "- **12 weather data files** (3 from each of 4 CSV files)\n",
        "- **12 extracted 24-hour files** (processed from downloaded files)\n",
        "- **24 hours of data** before peak flooding for each sample\n",
        "- **22 weather parameters** per file\n",
        "- **Complete metadata** (coordinates, peak date, hours before peak)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Processing Summary\n",
        "\n",
        "### First 3 Features Processing Pipeline\n",
        "\n",
        "This notebook downloads weather data for **the first 3 features from each CSV file**:\n",
        "\n",
        "#### Key Features\n",
        "- **Limited Processing**: Processes first 3 samples from each CSV file (12 total)\n",
        "- **API Optimization**: Uses optimized parameters with rate limiting handling\n",
        "- **Quality Assurance**: Zero null values, complete weather parameters\n",
        "- **Ready to Run**: No comments to uncomment, direct execution\n",
        "\n",
        "#### Expected Output\n",
        "- **12 weather data files** (3 from each of 4 CSV files)\n",
        "- **24 hours of data** before peak flooding for each sample\n",
        "- **22 weather parameters** per file\n",
        "- **Complete metadata** (coordinates, peak date, hours before peak)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
